{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W207.6 Final Project - Predicting Cancer Type from Tumor Mutations\n",
    "### Tony Di Sera, Vijay Singh, Rajiv Nair, Jeremey Fraenkel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this project, we analyze the tumor mutation dataset from PanCancer Atlas Initiative https://www.cell.com/pb-assets/consortium/pancanceratlas/pancani3/index.html. This is a cancer dataset comprising over 10,000 patients diagnosed with cancer.  Overall, the study collected diverse and detailed molecular information on each patient's tumor, including DNA sequencing.\n",
    "\n",
    "#### Primary Dataset\n",
    "The primary dataset we will be using is the somatic mutations file.  In addition, we may pull some patient features like gender and age at diagnosis from the clinical patient file.\n",
    "\n",
    "Number of Instances:  3,600,963 somatic mutations for 10,956 cancer patients\n",
    "Number of Attributes:  ~100 attributes for mutations, ~700 clinical attributes for patients. We will aggregate the mutation data by gene for each patient, reducing the number of attributes by patient to ~ 500-1000 features.\n",
    "\n",
    "#### Background\n",
    "By comparing the DNA from normal tissue cells to those of the cancerous cells, somatic mutations can be identified and characterized. Somatic mutations are non-inherited variations to the DNA of a cell that arise during an individual's lifetime. We will use these DNA mutations to predict cancer type, classified into 33 different tissue/organ types.  \n",
    "\n",
    "#### Motivation\n",
    "There is clinical value in being able to predict cancer type based on molecular profiles.  For some patients diagnosed with cancer, the biopsied tumor doesn't match the histologic characteristics of the organ/tissue site.  For example, a patient may have a liver tumor that cannot be characterized as liver cells when reviewed by the pathologist.  In these cases, the cancer may have originated from another site and has metastasized to the liver.  This is where genomic tumor data may provide insights by predicting the 'cell of origin', leading to a better-suited therapy for the patient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "from textwrap import wrap\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import time\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "#import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "from tensorflow.keras.layers import Dense as Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cd /content/drive/My Drive/berkeley/W207 machine learning/Final Project/w207_6_sum19_g5_final_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "For our analysis of cancer prediction using gene mutation and clinical data from patients, we will gather data from multiple sources. First we obtain the somatic mutation data from the PanCancerAtlas website (https://gdc.cancer.gov/about-data/publications/pancanatlas). We also download the patient clinical data that corresponds to the tumor data. At this time, we are not bringing in clinical features, but as the project progresses, we would like to bring in a few features from this clinical dataset (e.g. age a diagnosis, gender). In our notebook, we store this data locally so that it does not have to be downloaded if the notebook kernel is restarted and run multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the directory where the downloaded directory is stored\n",
    "data_dir = \"./data\"\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the somatic mutations file\n",
    "This file is in the 'MAF' file format, a bioinformatics tab separated format that can contains one record\n",
    "for each mutation observed in a patient tumor sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This downloads a 753 MB somatic mutations gzip file.  \n",
    "# This will take about 1-5 mins depending on your\n",
    "# connection speed.\n",
    "mutations_filename = \"./data/somatic_mutations.maf.gz\"\n",
    "if os.path.isfile(mutations_filename):\n",
    "    print(\"Skipping download, as file %s is present\" %(mutations_filename))\n",
    "else:\n",
    "    print('Downloading mutation data. 753 MB (may take a few minutes)...')\n",
    "    url = 'http://api.gdc.cancer.gov/data/1c8cfe5f-e52d-41ba-94da-f15ea1337efc'  \n",
    "    urllib.request.urlretrieve(url, mutations_filename)  \n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the patient clinical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This downloads an 18 MB patient clinical data file\n",
    "patient_filename = \"./data/patient_clinical_data.txt\"\n",
    "if os.path.isfile(patient_filename):\n",
    "    print(\"Skipping download, as file %s is present\" %(patient_filename))\n",
    "else:\n",
    "    print('Downloading clinical data ...')\n",
    "    url = 'http://api.gdc.cancer.gov/data/0fc78496-818b-4896-bd83-52db1f533c5c'  \n",
    "    urllib.request.urlretrieve(url, patient_filename)  \n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data dictionary\n",
    "All data source files are downloaded above.  This dataset, is a data dictionary\n",
    "that will allow us to translate cancer type codes to cancer type names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads the data dictionary to will convert\n",
    "# the tumor_sample_barcode into a cancer_type\n",
    "# and provide full names for the cancer types\n",
    "tcga_dict = open(\"./tcga_dictionaries.txt\",\"r\")\n",
    "dict_name_index = 0 #Set dictionary index counter to 0\n",
    "for line in tcga_dict:\n",
    "    if line.startswith(\"#\"): #If line starts with #, the next line will be a known dictionary\n",
    "        dict_name_index += 1\n",
    "    elif dict_name_index == 4:\n",
    "        tissue_source_site = eval(line)            \n",
    "    elif dict_name_index == 5:\n",
    "        code_to_disease = eval(line)\n",
    "    elif dict_name_index == 6:\n",
    "        disease_to_code = eval(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Gene Mutation Data ##\n",
    "\n",
    "Here we read the gene mutation data. This data file contains many columns, but after careful curation, we have decided to consider the following colums:\n",
    "\n",
    "1. **tumor_sample_barcode**: this contains the barcode with the first 12 characters identifying the patient\n",
    "2. **gene**: this is the actual gene that has been mutated (for e.g. TACC2, JAKMIP3, PANX3)\n",
    "3. **gene_type**: this indicates if the gene is protein coding or not.\n",
    "4. **chromosome**  **start** **end** **Strand**: the chromosome, start position and end position tells us the location of the gene where the mutation is seen.  Strand indicates if it is on the forward or reverse strand of the DNA.\n",
    "5. **variant_type**: this indicates if it is a single substitution mutation (SNP), a small deletion (DEL), or small insertion (INS), two nucleotide substitution (DNP), three nucleotide substitution (TNP), or more that three nucleotide substitution (ONP)\n",
    "6. **variant_classification**: this indicates what kind of molecular effect that this mutation will have on the protein.  The most common classes indicate if the substitution causes a change to the amino acid (missense vs silent).  Nonsense mutations cause premature termination of the protein; frameshift mutations cause a misreading of the amino acid sequence.\n",
    "7. **variant_impact**: this indicates how damaging the mutation -- HIGH, MODERATE, MODIFIER, or LOW.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mutations dataframe\n",
    "print('Loading mutations dataframe ...')\n",
    "\n",
    "mutations = pd.read_csv(mutations_filename, compression='gzip',\n",
    "                        sep='\\t',\n",
    "                        usecols=['Tumor_Sample_Barcode','Hugo_Symbol', 'BIOTYPE',\n",
    "                                'Chromosome', 'Start_Position',  'End_Position', 'Strand',\n",
    "                                'Variant_Type',  'Variant_Classification', 'IMPACT' ])\n",
    "\n",
    "print(\"done.\")\n",
    "\n",
    "# Set mutations index\n",
    "mutations['row'] = np.arange(len(mutations))\n",
    "mutations.set_index('row', inplace=True)\n",
    "\n",
    "# Rename the columns to more consistent names\n",
    "renamed_columns = { 'Tumor_Sample_Barcode': 'tumor_sample_barcode', \n",
    "                    'Hugo_Symbol': 'gene', \n",
    "                    'BIOTYPE': 'gene_type', \n",
    "                    'Chromosome': 'chromosome', \n",
    "                    'Start_Position': 'start', \n",
    "                    'End_Position': 'end', \n",
    "                    'Strand': 'strand', \n",
    "                    'Variant_Type': 'variant_type', \n",
    "                    'Variant_Classification': 'variant_classification', \n",
    "                    'IMPACT': 'variant_impact'}\n",
    "mutations.rename(renamed_columns, inplace=True, axis=1)\n",
    "\n",
    "\n",
    "print(\"\\nMutations count:       \", mutations.tumor_sample_barcode.count())\n",
    "print(\"Number of unique samples:\", mutations.tumor_sample_barcode.nunique())\n",
    "mutations.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual cancer type can be found by parsing the tumor sample barcode and then looking up\n",
    "the cancer type code in the dictionary based on the tissue source site portion of the\n",
    "tumor sample barcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the tissue source site from the tumor sample barcode.  Then use the\n",
    "# tissue site source to lookup the cancer type from the tcga_dictionaries\n",
    "def parse_cancer_type(tumor_sample_barcode):\n",
    "    tss = tumor_sample_barcode.split(\"-\")[1] #Extra the tissue source site from the tcga_id\n",
    "    cancer_type = disease_to_code[tissue_source_site[tss][1]][0] #Convert from tss to disease to code \n",
    "    return cancer_type\n",
    "\n",
    "\n",
    "mutations['cancer_type'] = mutations['tumor_sample_barcode'].apply(parse_cancer_type)\n",
    "print(\"Number of unique cancer types:\", mutations.cancer_type.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Patient Data##\n",
    "\n",
    "Here we load the clinical data. This is data for patients for whom we collected the gene mutation data above. The patients are identified by $patient\\_barcode$. We will use this field to populate the gene mutation data from the dataframe above in the table we are about to read. The clinical data has patient information such as gender and age at diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the clinical data\n",
    "print('Loading clinical dataframe ...')\n",
    "clinical = pd.read_csv(patient_filename, sep='\\t',\n",
    "                        usecols=['bcr_patient_barcode', 'acronym', 'gender', \n",
    "                                 'age_at_initial_pathologic_diagnosis'])\n",
    "\n",
    "# Rename the columns to more consistent names\n",
    "renamed_columns = { 'bcr_patient_barcode': 'patient_barcode', \n",
    "                    'acronym': 'cancer_type' }\n",
    "clinical.rename(renamed_columns, inplace=True, axis=1)\n",
    "\n",
    "print('Clinical count', clinical.patient_barcode.count())\n",
    "display(clinical.head(5))\n",
    "\n",
    "# Get cancer types\n",
    "cancer_types = clinical['cancer_type'].unique()\n",
    "print(\"\\nNumber of cancer types\", len(cancer_types))\n",
    "\n",
    "# Get number of cases per cancer type\n",
    "group_by_patient = clinical.groupby(['cancer_type'])['patient_barcode'].nunique()\n",
    "print(\"Number of patients\", group_by_patient.sum())\n",
    "group_by_patient.plot.bar(figsize=(12,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Merged Data ##\n",
    "\n",
    "Now that we have both gene and cancer data in one dataframe, and the patient clinical data in another dataframe, we will use the **patient_barcode** to merge these into a single table. With this, we can drop the tumor_sample_barcode column, since it has served its purpose. Looking at the data, it seems like some patient data is missing from the gene data. Simultaneously, some data in the gene dataframe does not have corresponding clinical data. Hence our merged dataframe size will be lower than the original mutations dataframe size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the patient barcode.  This is what we will use to join the mutations to the clinical data\n",
    "def parse_patient_barcode(tumor_sample_barcode):\n",
    "        return tumor_sample_barcode[0:12]\n",
    "\n",
    "\n",
    "mutations['patient_barcode'] = mutations['tumor_sample_barcode'].apply(parse_patient_barcode)\n",
    "mutations = mutations.drop(['tumor_sample_barcode'], axis=1)\n",
    "mutations = mutations.drop(['cancer_type'], axis=1)\n",
    "print(\"Number of unique patients:\", mutations['patient_barcode'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical['patient_barcode'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_count = 0\n",
    "gene_barcode_set = set(mutations.patient_barcode.unique())\n",
    "for bc in gene_barcode_set:\n",
    "    if bc not in set(clinical.patient_barcode.unique()):\n",
    "        missing_count += 1\n",
    "print(\"%d patients with gene data missing in clinical data\" %missing_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = mutations.merge(clinical, left_on='patient_barcode', right_on='patient_barcode')\n",
    "print('Merged mutations count:   ', merged.patient_barcode.count())\n",
    "print('Number of unique patients:', merged.patient_barcode.nunique())\n",
    "print('Number of cancer types:   ', merged.cancer_type.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminate non-coding genes. \n",
    "\n",
    "This is a common filter in bioinformatics analysis, eliminating genes that do not code for proteins.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate non-coding genes\n",
    "def eliminate_non_coding_genes(data):\n",
    "    data                       = data[data.gene_type == 'protein_coding']\n",
    "    return data\n",
    "\n",
    "before_count               = merged.gene.nunique()\n",
    "merged = eliminate_non_coding_genes(merged)\n",
    "after_count                = merged.gene.nunique()\n",
    "print(\"Filtered out \", str(before_count -after_count), \"genes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and test datasets\n",
    "Split the data into a training and test split.  We will use a split of 80% training, 20% test.  \n",
    "We will split based on the patient_barcode.  As part of feature engineering, we will be \n",
    "aggregating mutations, so that each example will be represented as a patient (tumor), with\n",
    "columns for each gene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Split the patients into training and test\n",
    "#\n",
    "def split_patient_data():\n",
    "    patient_data = merged.patient_barcode.unique()\n",
    "\n",
    "    le     = preprocessing.LabelEncoder()\n",
    "    patient_labels_string = merged.groupby('patient_barcode')['cancer_type'].nunique()\n",
    "    patient_labels = le.fit_transform(patient_labels_string)\n",
    "    \n",
    "    print(\"Number of unique patients:           \", patient_data.shape[0])\n",
    "    print(\"Number of labels for unique patients:\", len(patient_labels))\n",
    "    \n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(\n",
    "                                                               patient_data, patient_labels,\n",
    "                                                               stratify=patient_labels, \n",
    "                                                               test_size=0.20)\n",
    "\n",
    "\n",
    "\n",
    "    print(\"\\ntraining patients:  \", train_data.shape[0])\n",
    "    print(\"test patients:      \", test_data.shape[0])\n",
    "    return {'train_patients': train_data, 'test_patients': test_data}\n",
    "\n",
    "\n",
    "#\n",
    "#  Split Mutations data (based on patient split) and \n",
    "#  write out data files\n",
    "#\n",
    "def split_and_save_mutation_data():\n",
    "    split = split_patient_data()\n",
    "    train_patients = split['train_patients']\n",
    "    test_patients  = split['test_patients']\n",
    "\n",
    "    train_mutations = merged[merged.patient_barcode.isin(train_patients)]\n",
    "    test_mutations  = merged[merged.patient_barcode.isin(test_patients)]\n",
    "    print(\"\\ntraining data:      \", train_mutations.shape[0])\n",
    "    print(\"test data:          \", test_mutations.shape[0])\n",
    "    print(\"\\nall data:           \", merged.shape[0])\n",
    "    print(\"train + test:       \", train_mutations.shape[0] + test_mutations.shape[0])\n",
    "    \n",
    "    # Write out mutations training data as csv file\n",
    "    print(\"\\nWriting training set ...\")\n",
    "    train_mutations.to_csv(\"./data/somatic_mutations_train.csv\")\n",
    "    print(\"done.\")\n",
    "\n",
    "    # Write out mutations test data as csv file\n",
    "    print(\"\\nWriting test set ...\")\n",
    "    test_mutations.to_csv(\"./data/somatic_mutations_test.csv\")\n",
    "    print(\"done.\")\n",
    "\n",
    "split_and_save_mutation_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and Feature Selection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove old features_ files\n",
    "#filenames = glob.glob('./data/features_*.csv')\n",
    " \n",
    "# Iterate over the list of filepaths & remove each file.\n",
    "#for filePath in filenames:\n",
    "#    try:\n",
    "#        os.remove(filePath)\n",
    "#    except OSError:\n",
    "#        print(\"Error while deleting file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we open the data we put together in the previous notebook. For the first analysis, we look at $cancer\\_type$, $patient\\_barcode$, $gene$ and $gene_type$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading data ...')\n",
    "mutations = {}\n",
    "mutations['train'] = pd.read_csv(\"./data/somatic_mutations_train.csv\", \n",
    "                             usecols=['cancer_type', 'patient_barcode', 'gene', 'gene_type'])\n",
    "mutations['test']  = pd.read_csv(\"./data/somatic_mutations_test.csv\", \n",
    "                             usecols=['cancer_type', 'patient_barcode', 'gene', 'gene_type'])\n",
    "print(\"done.\")\n",
    "print(\"Mutations training data count:\", mutations['train']['patient_barcode'].count())\n",
    "print(\"Mutations test data count:    \", mutations['test']['patient_barcode'].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show distribution of genes across patient tumors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mutations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-457a9cec08a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Show the distribution of genes across patient tumors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgene_count\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0mmutations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gene'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'patient_barcode'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnunique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'count'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgene_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'gene'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'patient_count'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgene_count\u001b[0m         \u001b[0;34m=\u001b[0m \u001b[0mgene_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'patient_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gene'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Genes by patient frequency'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mutations' is not defined"
     ]
    }
   ],
   "source": [
    "# Show the distribution of genes across patient tumors\n",
    "gene_count         = mutations['train'].groupby(['gene'])['patient_barcode'].nunique().reset_index(name='count')\n",
    "gene_count.columns = ['gene', 'patient_count']\n",
    "gene_count         = gene_count.sort_values(['patient_count', 'gene'], ascending=[0,1])\n",
    "print('Genes by patient frequency')\n",
    "print(\"  mean:\", int(gene_count['patient_count'].mean()))\n",
    "print(\"  min: \", int(gene_count['patient_count'].min()))\n",
    "print(\"  max: \", int(gene_count['patient_count'].max()))\n",
    "\n",
    "ax = gene_count['patient_count'].hist(bins=200, figsize=(12,4))\n",
    "ax.set_xlabel(\"Number of Patients\")\n",
    "ax.set_ylabel(\"Gene Frequency\")\n",
    "plt.show()\n",
    "\n",
    "gene_cc_count         = mutations['train'].groupby(['gene'])['cancer_type'].nunique().reset_index(name='count')\n",
    "gene_cc_count.columns = ['gene', 'cancer_type_count']\n",
    "gene_cc_count         = gene_cc_count.sort_values(['cancer_type_count', 'gene'], ascending=[0,1])\n",
    "print('\\nGenes by cancer type frequency')\n",
    "print(\"  mean:\", int(gene_cc_count['cancer_type_count'].mean()))\n",
    "print(\"  min: \", int(gene_cc_count['cancer_type_count'].min()))\n",
    "print(\"  max: \", int(gene_cc_count['cancer_type_count'].max()))\n",
    "\n",
    "\n",
    "ax = gene_cc_count['cancer_type_count'].hist(bins=40, figsize=(12,4))\n",
    "ax.set_xlabel(\"Number of Cancer Types\")\n",
    "ax.set_ylabel(\"Gene Frequency\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histogram above, it is clear that even through we have a large number of genes, only a small number of them are turned on in the patient tumor data that we have. This is the classic problem of a large feature space with a much smaller number of samples. Hence we will need to perform a dimensionality reduction technique such as PCA here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the number of cancer types that are present in the \n",
    "# mutations dataset\n",
    "cancer_types = mutations['train'].cancer_type.unique()\n",
    "print(\"\\nNumber of cancer types:\", len(cancer_types))\n",
    "\n",
    "# Get number of cases per cancer type\n",
    "group_patients_by_cancer = mutations['train'].groupby(['cancer_type'])['patient_barcode'].nunique()\n",
    "print(\"\\nNumber of patients:\", group_patients_by_cancer.sum())\n",
    "group_patients_by_cancer.plot.bar(figsize=(12,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above chart shows that there are some cancers, such as BRCA and LUAD that have a large representation in our dataset, but other such as DBLC and UCS that are present in much smaller numbers. This will present a challenge for our classifier. Specifically, we want our classifier to be able to classify each of the 32 types of cancers with high precision, but the model should also be able to identify the cancers that don't have a proportionate representation in our data set. It could be that these are cancers are rare, or perhaps they are simply rare in our dataset. **Note:** add more details about the cancers that are abundant as well as rare in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique genes per cancer type\n",
    "group_genes_by_cancer = mutations['train'].groupby(['cancer_type'])['gene'].nunique();\n",
    "group_genes_by_cancer.plot.bar(figsize=(12,4))\n",
    "print(\"Mean number of genes represented for each cancer type:\", int(np.round(group_genes_by_cancer.mean())))\n",
    "print(\"Min number of genes represented for each cancer type: \", int(np.round(group_genes_by_cancer.min())))\n",
    "print(\"Max number of genes represented for each cancer type: \", int(np.round(group_genes_by_cancer.max())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above bar chart gives us an idea of how many genes (features for us) are _on_ for each of the cancer types. Cross referencing this chart with the previous one, we see that for some cancers such as DLBC and UCS we have a fair number of active features, even though the number of cases of such cancers are low. We should be able to person isolated (one-vs-rest) analysis for these cases. However, for other cancers, such as KICH (Kidney Chromophobe) and UVM (Uveal Melanoma) we have both a low occurance rate, and a low number of active features. This second category of cancers will need to be handled with care."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature matrix each row is a patient tumor; each column is a gene\n",
    "def create_patient_x_gene_matrix(mutations, feature_genes, description, save=True):\n",
    "    cases = list()\n",
    "    grouped = mutations.groupby('patient_barcode')\n",
    "    i = int(0)\n",
    "\n",
    "    cols = ['case_id', 'cancer_type']\n",
    "    for gene in feature_genes:\n",
    "        cols.append(gene)\n",
    "\n",
    "\n",
    "    for name, group in grouped:\n",
    "        case = list()\n",
    "        case.append(name)\n",
    "        for cc in group.cancer_type.head(1):\n",
    "            case.append(cc)\n",
    "\n",
    "        for gene_flag in feature_genes.isin(group.gene.unique()):\n",
    "            switch = 0\n",
    "            if gene_flag == True:\n",
    "                switch = 1\n",
    "            case.append(switch)\n",
    "        cases.append(case)\n",
    "\n",
    "\n",
    "    cases_df = pd.DataFrame(cases)\n",
    "    cases_df.columns = cols\n",
    "    print(\"  \", cases_df.shape)\n",
    "    \n",
    "    # Write out transformed data to csv\n",
    "    if save:\n",
    "        fileName = \"./data/\" + description + \".csv\"\n",
    "        print(\"  writing\", fileName, \"...\")\n",
    "        cases_df.to_csv(fileName)\n",
    "        print(\"  done.\")\n",
    "    \n",
    "    return cases_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_genes_across_cancer_types(top_gene_cancer_matrix, top_n_gene_count, total_gene_count):\n",
    "    plt.rcParams[\"figure.figsize\"] = (10,4)\n",
    "    sums_by_cancer_type = top_gene_cancer_matrix.sum(axis=1, skipna=True, numeric_only=True) \n",
    "    sorted = sums_by_cancer_type.sort_values(ascending=False).reindex()\n",
    "    df = pd.DataFrame(sorted).reset_index()\n",
    "    df.columns = ['gene', 'patient_count']\n",
    "    df.reset_index()    \n",
    "    title = 'Patient counts for genes (top ' + str(top_n_gene_count) + ')';\n",
    "    ax = df.head(50).plot.bar(x='gene', y='patient_count', legend=None, title=title)\n",
    "\n",
    "    cancer_type_present_count = top_gene_cancer_matrix.astype(bool).sum(axis=1, skipna=True, numeric_only=True)\n",
    "    sorted = cancer_type_present_count.sort_values(ascending=False).reindex()\n",
    "    df = pd.DataFrame(sorted).reset_index()\n",
    "    df.columns = ['gene', 'present_in_cancer_type_count']\n",
    "    df.reset_index()    \n",
    "\n",
    "    axarr = df.hist(bins=32)\n",
    "\n",
    "    for ax in axarr.flatten():\n",
    "        ax.set_xlabel(\"Number of cancer types gene is present\")\n",
    "        ax.set_ylabel(\"Gene frequency\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_feature_matrix(mutations_train, mutations_test, top_n_gene_count, save, description, charts=True):\n",
    "    print(\"Formatting gene matrix with top \", top_n_gene_count, \"genes from each cancer type\")\n",
    "    \n",
    "    # Now try to find the most common genes per cancer type and\n",
    "    # merge these together to come up with a master list\n",
    "    cancer_gene_count = mutations_train.groupby(['cancer_type', 'gene'])['patient_barcode'].nunique().reset_index(name='count')\n",
    "    cancer_gene_count.columns = ['cancer_type', 'gene', 'patient_count']\n",
    "\n",
    "\n",
    "\n",
    "    # Now create a large matrix, row is the gene, column for each cancer type\n",
    "    df = pd.DataFrame(cancer_gene_count, columns=['cancer_type', 'gene', 'patient_count'])\n",
    "    gene_cancer_matrix = pd.pivot_table(df, values='patient_count', index=['gene'],\n",
    "                         columns=['cancer_type'], aggfunc=np.sum, fill_value=0)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Now find the top n genes for each cancer type\n",
    "    top_genes = []\n",
    "    idx = 0\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "    for cancer_type in gene_cancer_matrix.columns:\n",
    "        sorted_genes = gene_cancer_matrix[cancer_type].sort_values(ascending=False)\n",
    "        top_rows = sorted_genes[sorted_genes > 0].head(top_n_gene_count)\n",
    "        for gene, patient_count in top_rows.items():\n",
    "            top_genes.append(list([cancer_type, gene, patient_count]))\n",
    "\n",
    "\n",
    "    # Turn this back into a matrix, row is gene, column for each cancer type\n",
    "    top_df = pd.DataFrame(top_genes, columns=['cancer_type', 'gene', 'patient_count'])\n",
    "    top_gene_cancer_matrix = pd.pivot_table(top_df, values='patient_count', index=['gene'],\n",
    "                         columns=['cancer_type'], aggfunc=np.sum, fill_value=0)\n",
    "    print(\"  number of genes:\", top_gene_cancer_matrix.shape[0])\n",
    "    if charts:\n",
    "        show_genes_across_cancer_types(top_gene_cancer_matrix, \n",
    "                                   top_n_gene_count, \n",
    "                                   top_gene_cancer_matrix.shape[0] )\n",
    "    feature_genes = top_gene_cancer_matrix.index\n",
    "    print(\"  number of genes after filter:\", len(feature_genes))\n",
    "    create_patient_x_gene_matrix(mutations_train, feature_genes, description + \".train\", save)\n",
    "    create_patient_x_gene_matrix(mutations_test,  feature_genes, description + \".test\", save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_best_fit_feature_matrix(mutations_train, mutations_test, top_n_genes, n_features, \n",
    "                               save, description, charts=True):\n",
    "    print(\"Formatting gene matrix with best fit for\", n_features, \"features\")\n",
    "    \n",
    "    # Now try to find the most common genes per cancer type and\n",
    "    # merge these together to come up with a master list\n",
    "    cancer_gene_count = mutations_train.groupby(['cancer_type', 'gene'])['patient_barcode'].nunique().reset_index(name='count')\n",
    "    cancer_gene_count.columns = ['cancer_type', 'gene', 'patient_count']\n",
    "\n",
    "\n",
    "\n",
    "    # Now create a large matrix, row is the gene, column for each cancer type\n",
    "    df = pd.DataFrame(cancer_gene_count, columns=['cancer_type', 'gene', 'patient_count'])\n",
    "    gene_cancer_matrix = pd.pivot_table(df, values='patient_count', index=['gene'],\n",
    "                         columns=['cancer_type'], aggfunc=np.sum, fill_value=0)\n",
    "    \n",
    "    \n",
    "\n",
    "    # Now find the top n genes for each cancer type\n",
    "    top_genes = []\n",
    "    for cancer_type in gene_cancer_matrix.columns:\n",
    "        sorted_genes = gene_cancer_matrix[cancer_type].sort_values(ascending=False)\n",
    "        if (top_n_genes == None):\n",
    "            top_rows = sorted_genes[sorted_genes > 0].head(top_n_genes)\n",
    "        else:\n",
    "            top_rows = sorted_genes\n",
    "        for gene, patient_count in top_rows.items():\n",
    "            top_genes.append(list([cancer_type, gene, patient_count]))\n",
    "\n",
    "\n",
    "    # Turn this back into a matrix, row is patient, column for each gene\n",
    "    top_df = pd.DataFrame(top_genes, columns=['cancer_type', 'gene', 'patient_count'])\n",
    "    top_gene_cancer_matrix = pd.pivot_table(top_df, values='patient_count', index=['gene'],\n",
    "                         columns=['cancer_type'], aggfunc=np.sum, fill_value=0)\n",
    "    show_genes_across_cancer_types(top_gene_cancer_matrix, \n",
    "                               top_n_genes, \n",
    "                               n_features )\n",
    "\n",
    "    #\n",
    "    # Create feature matrix, each row is patient, columns are genes\n",
    "    #\n",
    "    feature_genes = top_gene_cancer_matrix.index\n",
    "    print(\"  number of genes before best fit:     \", len(feature_genes))\n",
    "    feature_matrix_train = create_patient_x_gene_matrix(mutations_train, feature_genes, '', False)\n",
    "    feature_matrix_test  = create_patient_x_gene_matrix(mutations_test, feature_genes, '', False)\n",
    "    \n",
    "    #\n",
    "    #  Try BestFit (chi squared test) to find most\n",
    "    #  important genes\n",
    "    #\n",
    "    best_genes  = get_best_fit_features(feature_matrix_train, n_features)\n",
    "    cancer_type = feature_matrix_train['cancer_type']\n",
    "    case_id     = feature_matrix_train['case_id']\n",
    "    data_train  = feature_matrix_train.loc[:, feature_matrix_train.columns.isin(best_genes)]\n",
    "    \n",
    "    print(\"  number of genes before after best fit:\", data_train.shape[1])\n",
    "    final_feature_matrix_train = pd.concat([case_id, cancer_type, data_train], axis=1)\n",
    "\n",
    "    cancer_type = feature_matrix_test['cancer_type']\n",
    "    case_id     = feature_matrix_test['case_id']\n",
    "    data_test   = feature_matrix_test.loc[:, feature_matrix_test.columns.isin(best_genes)]\n",
    "    final_feature_matrix_test = pd.concat([case_id, cancer_type, data_test], axis=1)\n",
    "    \n",
    "    if save:\n",
    "        fileName = \"./data/\" + description  + \".train.csv\"\n",
    "        print(\"  writing\", fileName, \"...\")\n",
    "        print(\" \", final_feature_matrix_train.shape)\n",
    "        final_feature_matrix_train.to_csv(fileName)\n",
    "        print(\"  done.\")        \n",
    "        \n",
    "        fileName = \"./data/\" + description  + \".test.csv\"\n",
    "        print(\"  writing\", fileName, \"...\")\n",
    "        print(\" \", final_feature_matrix_test.shape)\n",
    "        final_feature_matrix_test.to_csv(fileName)\n",
    "        print(\"  done.\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_fit_features(feature_matrix, n_features):\n",
    "    #apply SelectKBest class to extract top n best features\n",
    "    bestfeatures = SelectKBest(score_func=chi2, k=n_features)\n",
    "    \n",
    "    data = feature_matrix.loc[:, (feature_matrix.columns != 'cancer_type') & (feature_matrix.columns != 'case_id')]\n",
    "    labels_string = feature_matrix['cancer_type']\n",
    "    \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    labels = le.fit_transform(labels_string)\n",
    "    \n",
    "    fit = bestfeatures.fit(data,labels)\n",
    "    dfscores = pd.DataFrame(fit.scores_)\n",
    "    dfcolumns = pd.DataFrame(data.columns)\n",
    "    \n",
    "    #concat two dataframes for better visualization \n",
    "    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "    featureScores.columns = ['gene','score']  \n",
    "    display(featureScores.nlargest(10,'score'))  \n",
    "    return list(featureScores.nlargest(n_features,'score')['gene'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_feature_matrix(mutations_train, mutations_test, save, description):\n",
    "    print(\"Formatting gene matrix with for all features\")\n",
    "    #\n",
    "    # Create feature matrix, each row is patient, columns are genes\n",
    "    #\n",
    "    feature_genes = pd.Series(mutations_train.gene.unique())\n",
    "    feature_matrix_train = create_patient_x_gene_matrix(mutations_train, feature_genes, description + \".train\", save)\n",
    "    feature_matrix_test  = create_patient_x_gene_matrix(mutations_test,  feature_genes, description + \".test\", save)\n",
    "    return feature_matrix_train, feature_matrix_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_l1_feature_matrix(train_features, test_features, label_encoder, description, save):\n",
    "    \n",
    "    train_first_cols    = train_features[train_features.columns[:2]]\n",
    "    train_data          = train_features[train_features.columns[3:]]\n",
    "    train_labels        = label_encoder.fit_transform(train_features.cancer_type)\n",
    "\n",
    "    test_first_cols    = test_features[test_features.columns[:2]]\n",
    "    test_data          = test_features[test_features.columns[3:]]\n",
    "    test_labels        = label_encoder.fit_transform(test_features.cancer_type)\n",
    "\n",
    "    params = {'C':  [100, 10, 1, .5]}\n",
    "    \n",
    "    for c_param in reversed(params['C']):\n",
    "        # Keep this random seed here to make comparison easier.\n",
    "        np.random.seed(0)\n",
    "\n",
    "        #\n",
    "        # Perform Logistic Regression on different C values\n",
    "        # using L1 regularization\n",
    "        #\n",
    "        l1 = LogisticRegression(penalty='l1', tol=.01, \n",
    "                            solver=\"liblinear\", multi_class=\"ovr\",\n",
    "                            max_iter=500, C=c_param)\n",
    "        # Fit model\n",
    "        l1.fit(train_data, train_labels) \n",
    "\n",
    "\n",
    "        # Get the features with non-zero coefficients.  We will use\n",
    "        # this list to reduce the features \n",
    "        non_zero_sums = np.where(np.sum(l1.coef_, axis=0) != 0)\n",
    "        names = np.array(list(train_data.columns))\n",
    "        non_zero_genes = names[non_zero_sums] \n",
    "\n",
    "\n",
    "        #\n",
    "        # Reduce feature size, only keeping features with non-zero weights \n",
    "        # found using l1 regularization\n",
    "        #\n",
    "        trimmed_train_data = train_data[non_zero_genes]\n",
    "        trimmed_test_data  = test_data[non_zero_genes]\n",
    "        \n",
    "        final_features_train = pd.concat([train_first_cols, trimmed_train_data], axis=1)\n",
    "        final_features_test =  pd.concat([test_first_cols, trimmed_test_data], axis=1)\n",
    "        \n",
    "        if save:\n",
    "            fileName = \"./data/\" + description + \"_c\" + str(c_param) + \".train.csv\"\n",
    "            print(\"  writing\", fileName, \"...\")\n",
    "            print(\" \", final_features_train.shape)\n",
    "            final_features_train.to_csv(fileName)\n",
    "            print(\"  done.\")        \n",
    "\n",
    "            fileName = \"./data/\" + description + \"_c\" + str(c_param) + \".test.csv\"\n",
    "            print(\"  writing\", fileName, \"...\")\n",
    "            print(\" \", final_features_test.shape)\n",
    "            final_features_test.to_csv(fileName)\n",
    "            print(\"  done.\")        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rfe_feature_matrix(train_features, test_features, label_encoder, description, save):\n",
    "    train_first_cols    = train_features[train_features.columns[:2]]\n",
    "    train_data          = train_features[train_features.columns[3:]]\n",
    "    train_labels        = label_encoder.fit_transform(train_features.cancer_type)\n",
    "\n",
    "    test_first_cols    = test_features[test_features.columns[:2]]\n",
    "    test_data          = test_features[test_features.columns[3:]]\n",
    "    test_labels        = label_encoder.fit_transform(test_features.cancer_type)\n",
    "\n",
    "    lr = LogisticRegression(penalty='l2', tol=.01, max_iter=150, \n",
    "                            C=0.25, \n",
    "                            solver=\"liblinear\", multi_class=\"ovr\")\n",
    "    rfe = RFE(estimator=lr, n_features_to_select=800, step=1)\n",
    "    rfe.fit(train_data, train_labels)\n",
    "    ranking = rfe.ranking_\n",
    "    print(ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create different feature sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top n genes most frequent in each cancer type\n",
    "\n",
    "Create a feature matrix, getting the top n genes that are most frequent\n",
    "per label (cancer type).  Merge these genes and create a feature matrix,\n",
    "one row per patient tumor, column for each merged gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_feature_matrix(mutations['train'], mutations['test'], 1000, True, 'features_topgenes_small')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use KBestFit to find best features (genes)\n",
    "\n",
    "Create a feature matrix, getting the top 1000 genes that are most frequent\n",
    "per label (cancer type).  Merge these genes. Then use sklearn BestFit to find \n",
    "top 5000 genes. Feature matrix will have one row per patient tumor, \n",
    "column for each 'bestfit' gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_best_fit_feature_matrix(mutations['train'], mutations['test'], 1000, 5000, True, \n",
    "                          'features_bestfit_with_topgenes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a feature matrix, using sklearn BestFit to find top \n",
    "5000 genes. Feature matrix will have one row per patient tumor, \n",
    "column for each 'bestfit' gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_best_fit_feature_matrix(mutations['train'], mutations['test'], None, 5000, True,  \n",
    "                           'features_bestfit_med')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a feature matrix, using sklearn BestFit to find top \n",
    "8000 genes. Feature matrix will have one row per patient tumor, \n",
    "column for each 'bestfit' gene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_best_fit_feature_matrix(mutations['train'], mutations['test'],None, 8000, True,  \n",
    "                           'features_bestfit_large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use all genes\n",
    "\n",
    "Create a feature matrix,  Feature matrix will have one row per patient tumor, \n",
    "column for every gene encountered in training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix_train, feature_matrix_test = create_all_feature_matrix(mutations['train'], \n",
    "                                                                   mutations['test'], True, 'features_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Trim the features using Recursive Feature Elimination\n",
    "#\n",
    "#label_encoder            = preprocessing.LabelEncoder()\n",
    "#create_rfe_feature_matrix(feature_matrix_train, feature_matrix_test, label_encoder, \n",
    "#                          'rfe_logit', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Logistic Regression with L1 regularization a different C values\n",
    "\n",
    "Trim the features using Logistic Regression, L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder            = preprocessing.LabelEncoder()\n",
    "create_l1_feature_matrix(feature_matrix_train, feature_matrix_test, label_encoder,\n",
    "           'features_l1reg', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use PCA for dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "all_features_train = feature_matrix_train.drop(columns=['case_id', 'cancer_type'])\n",
    "pca.fit(all_features_train)\n",
    "ev = np.cumsum(pca.explained_variance_ratio_)\n",
    "evcount = len(ev[ev<99.0])\n",
    "print(\"Number of features that explain 99% of the variance: \", evcount)\n",
    "pca = PCA(n_components=evcount)\n",
    "pca.fit(all_features_train)\n",
    "train_PCA = pca.transform(all_features_train)\n",
    "all_features_test = feature_matrix_test.drop(columns=['case_id', 'cancer_type'])\n",
    "test_PCA = pca.transform(all_features_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_PCA_df = pd.DataFrame(train_PCA)\n",
    "train_PCA_df['case_id'] = feature_matrix_train['case_id']\n",
    "train_PCA_df['cancer_type'] = feature_matrix_train['cancer_type']\n",
    "\n",
    "test_PCA_df = pd.DataFrame(test_PCA)\n",
    "test_PCA_df['case_id'] = feature_matrix_test['case_id']\n",
    "test_PCA_df['cancer_type'] = feature_matrix_test['cancer_type']\n",
    "\n",
    "#reorder columns and write out\n",
    "for df, f_name in zip( (train_PCA_df, test_PCA_df), \n",
    "                      ('./data/features_after_pca.train.csv', './data/features_after_pca.test.csv') ):\n",
    "  cols = df.columns.tolist()\n",
    "  cols = cols[-2:] + cols[:-2]\n",
    "  df = df[cols]\n",
    "  df.to_csv(f_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataAndLabels(name, features, label_encoder):\n",
    "    labels_string = features.cancer_type\n",
    "   \n",
    "    labels        = label_encoder.fit_transform(labels_string)\n",
    "\n",
    "    # Get rid of the cancer type and patient_barcode columns \n",
    "    data = features[features.columns[3:]]\n",
    "\n",
    "    return {'name': name, 'feature_size': data.shape[1],\n",
    "            'data': data, 'labels': labels , 'label_encoder': label_encoder }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data ...\n",
      "  l1reg_c1\n",
      "  l1reg_c100\n",
      "  l1reg_c0.5\n",
      "  l1reg_c10\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "print('Loading training data ...')\n",
    "# label encoder\n",
    "label_encoder   = preprocessing.LabelEncoder()\n",
    "\n",
    "# get all file names that start with features_\n",
    "train_files = glob.glob(\"./data/features_l1reg_c*.train.csv\")\n",
    "all_train_data = {}\n",
    "\n",
    "# load all of the files\n",
    "for filename in train_files:\n",
    "    \n",
    "    name = filename[16:-10]\n",
    "    print(\" \", name)\n",
    "    train_features = pd.read_csv(filename)\n",
    "    all_train_data[name] = getDataAndLabels(name, train_features, label_encoder)\n",
    "\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data ...\n",
      "  l1reg_c10\n",
      "  l1reg_c0.5\n",
      "  l1reg_c100\n",
      "  l1reg_c1\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "print('Loading test data ...')\n",
    "\n",
    "test_files = glob.glob(\"./data/features_l1reg_c*.test.csv\")\n",
    "all_test_data = {}\n",
    "for filename in test_files:\n",
    "    \n",
    "    name = filename[16:-9]\n",
    "    #if (name != 'after_pca'):\n",
    "    print(\" \", name)\n",
    "    test_features = pd.read_csv(filename)\n",
    "    all_test_data[name] = getDataAndLabels(name, test_features, label_encoder)\n",
    "\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for running different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestParamsLogit(train_data, train_labels):\n",
    "    #\n",
    "    # Logistic Regression\n",
    "    #\n",
    "    lr = LogisticRegression(penalty='l2', multi_class = 'ovr', solver='liblinear', max_iter=150)\n",
    "    params = {'C': [0.1, 0.25,  0.5,]}\n",
    "    logit = GridSearchCV(lr, params, cv=5,\n",
    "                         scoring='accuracy', return_train_score=True)\n",
    "\n",
    "    # Fit  training data\n",
    "    logit.fit(train_data, train_labels)  \n",
    "    # Show the best C parameter to use and the expected accuracy\n",
    "    print(' Best param:', logit.best_params_)\n",
    "    print(' Accuracy:  ', np.round(logit.best_score_, 4) )\n",
    "    \n",
    "    return logit.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestParamsSVM(train_data, train_labels):\n",
    "    #\n",
    "    # SVM\n",
    "    #\n",
    "    classifier = LinearSVC(penalty='l2')\n",
    "\n",
    "    params = {'C': [0.01, 0.1, 0.5]}\n",
    "    svm = GridSearchCV(classifier, params, cv=4, \n",
    "                       scoring='accuracy', return_train_score=True)\n",
    "\n",
    "    # Fit  training data\n",
    "    svm.fit(train_data, train_labels)  \n",
    "    # Show the best C parameter to use and the expected accuracy\n",
    "    print(' Best param:', svm.best_params_)\n",
    "    print(' Accuracy:  ', np.round(svm.best_score_, 4) )\n",
    "    \n",
    "    return svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Logistic regression\n",
    "#\n",
    "def run_logistic_regression(train_data, train_labels, test_data, test_labels, name, hyper_params):\n",
    "    start = time.process_time()\n",
    "    if name in hyper_params and 'lr' in hyper_params[name]:\n",
    "        best_params_logit = hyper_params[name]['lr']\n",
    "    else:\n",
    "        print(\"Running grid search on Logistic Regression...\")\n",
    "        best_params_logit = getBestParamsLogit(train_data, train_labels)\n",
    "\n",
    "    # Run logistic regression with L2 regularization on reduced\n",
    "    # feature set\n",
    "    lr = LogisticRegression(penalty='l2', tol=.01, max_iter=150, \n",
    "                          C=best_params_logit['C'], \n",
    "                          solver=\"liblinear\", multi_class=\"ovr\")\n",
    "    lr.fit(train_data, train_labels) \n",
    "    predict = lr.predict(test_data)\n",
    "    elapsed_time = time.process_time() - start\n",
    "\n",
    "    # Get precision, recall, f1 scores\n",
    "    logit_prf_scores      = precision_recall_fscore_support(test_labels, predict, average='weighted')\n",
    "    logit_scores_by_label = precision_recall_fscore_support(test_labels, predict, average=None)\n",
    "\n",
    "    # Get confusion matrix\n",
    "    logit_confusion       = confusion_matrix(test_labels, predict)\n",
    "\n",
    "    print(\"\\nLogistic Regression\", name)\n",
    "    print(\"  precision:\", np.round(logit_prf_scores[0], 4))  \n",
    "    print(\"  recall:   \", np.round(logit_prf_scores[1], 4))  \n",
    "    print(\"  f1:       \", np.round(logit_prf_scores[2], 4))   \n",
    "    print(\"  time:     \", np.round(elapsed_time, 2))      \n",
    "    return {'scores': [\n",
    "              logit_prf_scores[0],\n",
    "              logit_prf_scores[1],\n",
    "              logit_prf_scores[2],\n",
    "              logit_scores_by_label,\n",
    "              logit_confusion], \n",
    "          'time': elapsed_time }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Linear SVM\n",
    "#\n",
    "def run_linear_svm(train_data, train_labels, test_data, test_labels, name, hyper_params):\n",
    "    print(\"\\nLinear SVM\", name)\n",
    "    start = time.process_time()\n",
    "    if name in hyper_params and 'svm' in hyper_params[name]:\n",
    "        best_params_svm = hyper_params[name]['svm']\n",
    "    else:\n",
    "        print(\"Running grid search on Linear SVM...\")\n",
    "        best_params_svm = getBestParamsSVM(train_data, train_labels)\n",
    "\n",
    "    svm = LinearSVC(penalty='l2', C=best_params_svm['C'])\n",
    "\n",
    "    svm.fit(train_data, train_labels,) \n",
    "    predict = svm.predict(test_data)\n",
    "    elapsed_time = time.process_time() - start\n",
    "\n",
    "    # Get precision, recall, f1 scores\n",
    "    svm_prf_scores      = precision_recall_fscore_support(test_labels, predict, average='weighted')\n",
    "    svm_scores_by_label = precision_recall_fscore_support(test_labels, predict, average=None)\n",
    "\n",
    "    # Get confusion matrix\n",
    "    svm_confusion       = confusion_matrix(test_labels, predict)\n",
    "\n",
    "    print(\"  precision:\", np.round(svm_prf_scores[0], 4))  \n",
    "    print(\"  recall:   \", np.round(svm_prf_scores[1], 4))  \n",
    "    print(\"  f1:       \", np.round(svm_prf_scores[2], 4))      \n",
    "    print(\"  time:     \", np.round(elapsed_time, 2))      \n",
    "\n",
    "    return {'scores': [\n",
    "              svm_prf_scores[0],\n",
    "              svm_prf_scores[1],\n",
    "              svm_prf_scores[2],\n",
    "              svm_scores_by_label,\n",
    "              svm_confusion], \n",
    "          'time': elapsed_time }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Decision tree\n",
    "#\n",
    "def run_decision_tree(train_data, train_labels, test_data, test_labels, name, hyper_params):\n",
    "\n",
    "    print(\"\\nDecision Tree\", name)\n",
    "    start = time.process_time()\n",
    "    dt = DecisionTreeClassifier()\n",
    "    \n",
    "    dt.fit(train_data, train_labels,) \n",
    "    predict = dt.predict(test_data)\n",
    "    elapsed_time = time.process_time() - start\n",
    "\n",
    "\n",
    "    # Get precision, recall, f1 scores\n",
    "    dt_prf_scores      = precision_recall_fscore_support(test_labels, predict, average='weighted')\n",
    "    dt_scores_by_label = precision_recall_fscore_support(test_labels, predict, average=None)\n",
    "\n",
    "    # Get confusion matrix\n",
    "    dt_confusion       = confusion_matrix(test_labels, predict)\n",
    "\n",
    "    \n",
    "    print(\"  precision:\", np.round(dt_prf_scores[0], 4))  \n",
    "    print(\"  recall:   \", np.round(dt_prf_scores[1], 4))  \n",
    "    print(\"  f1:       \", np.round(dt_prf_scores[2], 4))\n",
    "    print(\"  time:     \", np.round(elapsed_time, 2))          \n",
    " \n",
    "    return {'scores': [\n",
    "              dt_prf_scores[0],\n",
    "              dt_prf_scores[1],\n",
    "              dt_prf_scores[2],\n",
    "              dt_scores_by_label,\n",
    "              dt_confusion], \n",
    "          'time': elapsed_time }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Random forest\n",
    "#\n",
    "def run_random_forest(train_data, train_labels, test_data, test_labels, name, hyper_params):\n",
    "    print(\"\\nRandom Forest\", name)\n",
    "    start = time.process_time()\n",
    "    rf = RandomForestClassifier(n_estimators=500)\n",
    "    \n",
    "    rf.fit(train_data, train_labels,) \n",
    "    predict = rf.predict(test_data)\n",
    "    elapsed_time = time.process_time() - start\n",
    "\n",
    "\n",
    "    # Get precision, recall, f1 scores\n",
    "    rf_prf_scores      = precision_recall_fscore_support(test_labels, predict, average='weighted')\n",
    "    rf_scores_by_label = precision_recall_fscore_support(test_labels, predict, average=None)\n",
    "\n",
    "    # Get confusion matrix\n",
    "    rf_confusion       = confusion_matrix(test_labels, predict)\n",
    "    \n",
    "    print(\"  precision:\", np.round(rf_prf_scores[0], 4))  \n",
    "    print(\"  recall:   \", np.round(rf_prf_scores[1], 4))  \n",
    "    print(\"  f1:       \", np.round(rf_prf_scores[2], 4)) \n",
    "    print(\"  time:     \", np.round(elapsed_time, 2))      \n",
    "    \n",
    "    return {'scores': [\n",
    "              rf_prf_scores[0],\n",
    "              rf_prf_scores[1],\n",
    "              rf_prf_scores[2],\n",
    "              rf_scores_by_label,\n",
    "              rf_confusion], \n",
    "          'time': elapsed_time }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Neural Net\n",
    "#\n",
    "def run_neural_net(train_data, train_labels, test_data, test_labels, name, hyper_params):\n",
    "    print(\"\\nNeural Net\", name)\n",
    "    tr_lab = to_categorical(train_labels)\n",
    "    test_lab = to_categorical(test_labels)\n",
    "    start = time.process_time()\n",
    "    model = K.Sequential()\n",
    "    model.add(Dense(2000, input_dim=train_data.shape[1], activation='relu', \n",
    "                    kernel_regularizer=regularizers.l1_l2(l2=0.01,l1=0.01)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1000, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(400, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = [\"accuracy\"])\n",
    "    #model.fit(train_data, tr_lab, epochs=1000, batch_size=100)\n",
    "    model.fit(train_data, tr_lab, epochs=1, batch_size=100)\n",
    "    evaluate = model.evaluate(x = test_data, y = test_lab)\n",
    "    predict = model.predict(test_data)    \n",
    "    elapsed_time = time.process_time() - start\n",
    "    # Get precision, recall, f1 scores\n",
    "    nn_prf_scores      = precision_recall_fscore_support(test_labels,np.argmax(predict,1), average='weighted')\n",
    "    nn_scores_by_label = precision_recall_fscore_support(test_labels,np.argmax(predict,1), average=None)\n",
    "\n",
    "    # Get confusion matrix\n",
    "    #nn_confusion       = confusion_matrix(test_labels, predict)\n",
    "    \n",
    "    print(\"  precision:\", np.round(nn_prf_scores[0], 4))  \n",
    "    print(\"  recall:   \", np.round(nn_prf_scores[1], 4))  \n",
    "    print(\"  f1:       \", np.round(nn_prf_scores[2], 4))  \n",
    "    print(\"  time:     \", np.round(elapsed_time, 2))      \n",
    "    \n",
    "    return {'scores': [\n",
    "              nn_prf_scores[0],\n",
    "              nn_prf_scores[1],\n",
    "              nn_prf_scores[2],\n",
    "              nn_scores_by_label,\n",
    "              []],  ## TODO Jeremy:  add confusion matrix\n",
    "           'time': elapsed_time}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# XGBoost\n",
    "#\n",
    "def run_xg_boost(train_data, train_labels, test_data, test_labels, name, hyper_params):\n",
    "    print(\"\\nXG Boost\", name)\n",
    "    start = time.process_time()\n",
    "\n",
    "    xgb_params = {\n",
    "    'max_depth': 2, \n",
    "    'eta': 0.3,  \n",
    "    'silent': False,  \n",
    "    'verbose': True,\n",
    "    'objective': 'multi:softprob',  \n",
    "    'num_class': 32,\n",
    "    'num_boost_round' : 2}  \n",
    "\n",
    "    xgb_cfr = xgb.XGBClassifier(**xgb_params)\n",
    "    xgb_cfr.fit(train_data, train_labels)\n",
    "    \n",
    "    predict = xgb_cfr.predict(test_ata)\n",
    "    elapsed_time = time.process_time() - start\n",
    "\n",
    "    \n",
    "    # Get precision, recall, f1 scores\n",
    "    xgb_prf_scores      = precision_recall_fscore_support(test_labels, predict, average='weighted')\n",
    "    xgb_scores_by_label = precision_recall_fscore_support(test_labels, predict, average=None)\n",
    "\n",
    "    # Get confusion matrix\n",
    "    xgb_confusion       = confusion_matrix(test_labels, predict)\n",
    "    \n",
    "    print(\"  precision:\", np.round(xgb_prf_scores[0], 4))  \n",
    "    print(\"  recall:   \", np.round(xgb_prf_scores[1], 4))  \n",
    "    print(\"  f1:       \", np.round(xgb_prf_scores[2], 4))\n",
    "    print(\"  time:     \", np.round(elapsed_time, 8))      \n",
    "    \n",
    "    return {'scores': [\n",
    "              xgb_prf_scores[0],\n",
    "              xgb_prf_scores[1],\n",
    "              xgb_prf_scores[2],\n",
    "              xgb_scores_by_label,\n",
    "              xgb_confusion], \n",
    "          'time': elapsed_time}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the different classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_saved_metrics():\n",
    "    metrics_filename = \"./data/metrics.csv\"\n",
    "    if os.path.isfile(metrics_filename):\n",
    "        df_report_existing = pd.read_csv(metrics_filename)\n",
    "        return df_report_existing\n",
    "    else:\n",
    "        return pd.DataFrame(columns=['name', 'classifier', 'feature_size', 'precision', 'recall', 'f1', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metrics(scores, times):\n",
    "    df_scores = pd.DataFrame(scores)\n",
    "    df_times = pd.DataFrame(times)\n",
    "    rows = []\n",
    "\n",
    "    for classifier in df_scores.index:\n",
    "        for name in df_scores.columns: \n",
    "            rows.append([name,\n",
    "                         all_train_data[name]['feature_size'],\n",
    "                        classifier,\n",
    "                        df_scores.loc[classifier][name][0],\n",
    "                        df_scores.loc[classifier][name][1],\n",
    "                        df_scores.loc[classifier][name][2],\n",
    "                        df_times.loc[classifier][0]])\n",
    "\n",
    "    df_report = pd.DataFrame(rows, columns=['name', 'feature_size', 'classifier', 'precision', 'recall', 'f1', 'time'])\n",
    "\n",
    "    # Write out scores as csv files\n",
    "    print(\"\\nWriting metrics ...\")\n",
    "    df_report.to_csv(\"./data/metrics.csv\")\n",
    "    print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    'l1reg_c0.5':           {'lr': {'C': 0.25}, 'svm': {'C': 0.01}},\n",
    "    'l1reg_c1':             {'lr': {'C': 0.25}, 'svm': {'C': 0.01}},\n",
    "    'l1reg_c10':            {'lr': {'C': 0.1},  'svm': {'C': 0.01}},\n",
    "    'l1reg_c100':           {'lr': {'C': 0.25}, 'svm': {'C': 0.01}},\n",
    "    'topgenes_small':       {'lr': {'C': 0.25}, 'svm': {'C': 0.01}},\n",
    "    'bestfit_med':          {'lr': {'C': 0.1 }, 'svm': {'C': 0.01}},\n",
    "    'bestfit_large':        {'lr': {'C': 0.1 }, 'svm': {'C': 0.01}},\n",
    "    'all':                  {'lr': {'C': 0.25}, 'svm': {'C': 0.01}},\n",
    "    'bestfit_with_topgenes':{'lr': {'C': 0.1 }, 'svm': {'C': 0.01}},\n",
    "    'after_pca':            {'lr': {'C': 0.5 }, 'svm': {'C': 0.01}}\n",
    "}\n",
    "\n",
    "def is_existing_run(existing_runs, the_classifier, the_name):\n",
    "    matches  = existing_runs[(existing_runs.classifier == the_classifier) & (existing_runs.name == the_name)]\n",
    "    return matches.shape[0]\n",
    "\n",
    "def copy_metrics(existing_runs, the_classifier, the_name):\n",
    "    matches  = existing_runs[(existing_runs.classifier == the_classifier) & (existing_runs.name == the_name)]\n",
    "    scores[the_name][the_classifier] = [matches.precision.values[0], matches.recall.values[0], matches.f1.values[0]]\n",
    "    times[the_name][the_classifier]  = matches.time.values[0]\n",
    "\n",
    "def run_classifiers(train_data, train_labels, test_data, test_labels, name, scores, times, existing_runs):\n",
    "    scores[name] = {}\n",
    "    times[name] = {}\n",
    "\n",
    "    if existing_run(existing_runs, 'lr', name) == 0:\n",
    "        lr = run_logistic_regression(train_data, train_labels, test_data, test_labels, name, hyper_params)\n",
    "        scores[name]['lr']  = lr['scores']\n",
    "        times[name]['lr']  = lr['time']\n",
    "    else:\n",
    "        copy_metrics(existing_runs, 'lr', name)\n",
    "\n",
    "\n",
    "    if existing_run(existing_runs, 'svm', name) == 0:\n",
    "        svm = run_linear_svm(train_data, train_labels, test_data, test_labels, name, hyper_params)\n",
    "        scores[name]['svm'] = svm['scores']\n",
    "        times[name]['svm'] = svm['time']\n",
    "    else:\n",
    "        copy_metrics(existing_runs, 'svm', name)\n",
    "\n",
    "        \n",
    "    if existing_run(existing_runs, 'dt', name) == 0:\n",
    "        dt = run_decision_tree(train_data, train_labels, test_data, test_labels, name, hyper_params)\n",
    "        scores[name]['dt']  = dt['scores']\n",
    "        times[name]['dt']  = dt['time']\n",
    "    else:\n",
    "        copy_metrics(existing_runs, 'dt', name)\n",
    "\n",
    "\n",
    "    if existing_run(existing_runs, 'rf', name) == 0:\n",
    "        rf = run_random_forest(train_data, train_labels, test_data, test_labels, name, hyper_params)\n",
    "        scores[name]['rf']  = rf['scores']\n",
    "        times[name]['rf']  = rf['time']\n",
    "    else:\n",
    "        copy_metrics(existing_runs, 'rf', name)\n",
    "\n",
    "       \n",
    "    if existing_run(existing_runs, 'nn', name) == 0:\n",
    "        nn = run_neural_net(train_data, train_labels, test_data, test_labels, name, hyper_params)\n",
    "        scores[name]['nn']  = nn['scores']\n",
    "        times[name]['nn']  = nn['time']\n",
    "    else:\n",
    "        copy_metrics(existing_runs, 'nn', name) \n",
    "        \n",
    "        \n",
    "    save_metrics(scores, times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************\n",
      "l1reg_c1\n",
      "************************\n",
      "\n",
      "Writing metrics ...\n",
      "done.\n",
      "************************\n",
      "l1reg_c100\n",
      "************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonyd/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression l1reg_c100\n",
      "  precision: 0.5503\n",
      "  recall:    0.5435\n",
      "  f1:        0.5214\n",
      "  time:      101.01\n",
      "\n",
      "Linear SVM l1reg_c100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonyd/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  precision: 0.5387\n",
      "  recall:    0.5445\n",
      "  f1:        0.5206\n",
      "  time:      9.04\n",
      "\n",
      "Decision Tree l1reg_c100\n",
      "  precision: 0.3478\n",
      "  recall:    0.3497\n",
      "  f1:        0.3429\n",
      "  time:      63.15\n",
      "\n",
      "Random Forest l1reg_c100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-182-4e97fa9a2d82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     run_classifiers(train['data'], train['labels'], test['data'], test['labels'], name, scores, times, \n\u001b[0;32m---> 17\u001b[0;31m                     existing_runs)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-181-5ffe54bb57f0>\u001b[0m in \u001b[0;36mrun_classifiers\u001b[0;34m(train_data, train_labels, test_data, test_labels, name, scores, times, existing_runs)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexisting_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexisting_runs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rf'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_random_forest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyper_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rf'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scores'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mtimes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rf'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-7dcf810da462>\u001b[0m in \u001b[0;36mrun_random_forest\u001b[0;34m(train_data, train_labels, test_data, test_labels, name, hyper_params)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mrf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Validate or convert input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 542\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# safely to reduce dtype induced overflows.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mis_float\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m'fc'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mis_float\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_safe_accumulator_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_float\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36m_safe_accumulator_op\u001b[0;34m(op, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    684\u001b[0m     \"\"\"\n\u001b[1;32m    685\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemsize\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial)\u001b[0m\n\u001b[1;32m   2074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0;32m-> 2076\u001b[0;31m                           initial=initial)\n\u001b[0m\u001b[1;32m   2077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "scores = {}\n",
    "times  = {}\n",
    "\n",
    "existing_runs = get_saved_metrics()\n",
    "\n",
    "for name in all_train_data.keys():\n",
    "    print(\"************************\")\n",
    "    print(name)\n",
    "    print(\"************************\")\n",
    "\n",
    "    train      = all_train_data[name]\n",
    "    test       = all_test_data[name]\n",
    "\n",
    "    run_classifiers(train['data'], train['labels'], test['data'], test['labels'], name, scores, times, \n",
    "                    existing_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Performance across different feature sets, different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {'lr': 'olivedrab', 'svm': 'slateblue', \n",
    "          'dt': 'mediumseagreen', 'rf': 'goldenrod',\n",
    "          'xgb': 'coral', 'nn': 'crimson'}\n",
    "\n",
    "def plot_classifier_metrics(df_report, label_encoder):\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "\n",
    "    labels = []\n",
    "    for key, group in df_report.groupby(['feature_size', 'name']):\n",
    "        labels.append(str(key[0]) + '\\n' + key[1])\n",
    "        \n",
    "    sorted_df_report = df_report.sort_values(by=['classifier', 'feature_size', 'name'], ascending=[1,1,1])\n",
    "\n",
    "\n",
    "        \n",
    "    for classifier, group in sorted_df_report.groupby(['classifier']):\n",
    "\n",
    "        plt.plot(labels, group.precision.values, color=colors[classifier], \n",
    "                 linewidth=3, label=classifier + \" precision\", marker='o' )\n",
    "        plt.plot(labels, group.recall.values, color=colors[classifier], linestyle=\"dashed\",\n",
    "                 linewidth=3, label=classifier + \" recall\", marker='o' )\n",
    "    \n",
    "\n",
    "    plt.yticks(np.arange(0, .65, .01))\n",
    "    plt.ylabel('Precision, Recall', fontsize=20)\n",
    "    plt.xlabel('Precision and Recall across different Features and Classifiers', fontsize=20, labelpad=20)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classifier_times(df_report, label_encoder):\n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = (10,10)\n",
    "\n",
    "    labels = []\n",
    "    for key, group in df_report.groupby(['feature_size', 'name']):\n",
    "        labels.append(str(key[0]) + '\\n' + key[1])\n",
    "        \n",
    "    sorted_df_report = df_report.sort_values(by=['classifier', 'feature_size', 'name'], ascending=[1,1,1])\n",
    "\n",
    "\n",
    "        \n",
    "    for classifier, group in sorted_df_report.groupby(['classifier']):\n",
    "\n",
    "        plt.plot(labels, group.time.values, color=colors[classifier], \n",
    "                 linewidth=3, label=classifier + \" precision\", marker='o' )\n",
    "        \n",
    "    \n",
    "\n",
    "    plt.ylabel('Time to train and predict', fontsize=20)\n",
    "    plt.xlabel('Run times across different Features and Classifiers', fontsize=20, labelpad=20)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_precision_recall_by_label(precision_by_label, recall_by_label, name, classifier, label_encoder):\n",
    "\n",
    "    labels = []\n",
    "    for i in range(len(precision_by_label)):\n",
    "        label = label_encoder.inverse_transform([i])[0]\n",
    "        labels.append(label)\n",
    "    \n",
    "    y_pos = np.arange(len(labels))    \n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=False)\n",
    "\n",
    "    ax1.invert_xaxis()\n",
    "    ax1.yaxis.tick_right()\n",
    "    \n",
    "    ax1.set_yticks(y_pos)\n",
    "    ax1.set_yticklabels(labels)\n",
    "    \n",
    "    ax2.set_yticks(y_pos)\n",
    "    ax2.set_yticklabels(labels)\n",
    "        \n",
    "    ax1.barh(y_pos, precision_by_label, color=colors[classifier] , label=\"precision\")\n",
    "    ax2.barh(y_pos, recall_by_label,    color=colors[classifier],  label='recall')\n",
    "\n",
    "    ax1.set_title('Precision( ' + classifier + ')')\n",
    "    ax2.set_title('Recall (' + classifier + ')')\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coords_of_max(theArray, n):\n",
    "    # Flatten the 2D array\n",
    "    flat = theArray.flatten()\n",
    "    # Partition so that the we know the sort order for\n",
    "    # the cells with the highest values.  We just\n",
    "    # care about the top n highest values.  So for example,\n",
    "    # if n = 3, get return 3 indices.  \n",
    "    indices = np.argpartition(flat, -n)[-n:]\n",
    "    # Reverse so that we show index of highest value first\n",
    "    # (descending)\n",
    "    indices = indices[np.argsort(-flat[indices])]\n",
    "    # Now return the coordinates for these indices\n",
    "    # for a 2D array.  This will return 2 arrays,\n",
    "    # the first for the row index, the second for the\n",
    "    # column index.  The row index represents the\n",
    "    # actual digit, the column index represents\n",
    "    # the confused digit\n",
    "    return np.unravel_index(indices, theArray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(conf_mx, label_encoder):\n",
    "    # Determine the error rates for each misclassification pair\n",
    "    row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "    norm_conf_mx = conf_mx / row_sums\n",
    "    # Set the error rates for correctly classified pairs (the diagonal) to zero\n",
    "    np.fill_diagonal(norm_conf_mx, 0)\n",
    "    \n",
    "    max_coords = coords_of_max(norm_conf_mx, 20)\n",
    "    confusion_rows = []\n",
    "    for i in range(len(max_coords[0])):\n",
    "\n",
    "        # This is the actual label\n",
    "        actual_label_idx  = max_coords[0][i]\n",
    "        actual_label      = label_encoder.inverse_transform([actual_label_idx])[0]\n",
    "\n",
    "        # This is the predicted label\n",
    "        predicted_label_idx = max_coords[1][i]\n",
    "        predicted_label = label_encoder.inverse_transform([predicted_label_idx])[0]\n",
    "        \n",
    "        # This is the error rate\n",
    "        error_rate  = norm_conf_mx[max_coords[0][i], max_coords[1][i]]\n",
    "        error_count = conf_mx[max_coords[0][i], max_coords[1][i]]\n",
    "\n",
    "        row = list([ actual_label,                     \n",
    "                     predicted_label,\n",
    "                     code_to_disease[actual_label][0], \n",
    "                     code_to_disease[predicted_label][0], \n",
    "                     error_rate, \n",
    "                     error_count ])\n",
    "        confusion_rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(confusion_rows, columns=['actual', 'predicted',  'actual_name', 'predicted_name', 'error_rate', 'error_count'])\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot precision metrics across different classifiers and feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precision and accuracy across different classifiers\n",
    "plot_classifier_metrics(df_report, label_encoder)\n",
    "\n",
    "# Plot time across different classifiers\n",
    "plot_classifier_times(df_report, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the precision, recall, and f1 score across different classifiers and feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best precision\n",
    "sorted_df = df_report.sort_values(by='precision', ascending=0)\n",
    "best_precision = sorted_df.head(1)\n",
    "\n",
    "# best recall\n",
    "sorted_df = df_report.sort_values(by='recall', ascending=0)\n",
    "best_recall = sorted_df.head(1)\n",
    "\n",
    "# best f1\n",
    "sorted_df = df_report.sort_values(by='f1', ascending=0)\n",
    "best_f1 = sorted_df.head(1)\n",
    "\n",
    "# Show the feature set and classifier with the best \n",
    "# precision, recall, and f1 scores\n",
    "print(\"\\n\\nBest precision\")\n",
    "display(best_precision)\n",
    "print(\"\\n\\nBest recall\")\n",
    "display(best_recall)\n",
    "print(\"\\n\\nBest f1\")\n",
    "display(best_f1)\n",
    "\n",
    "# get the scores by label and confusion matrix\n",
    "# for the best prediction\n",
    "best_prediction = best_precision\n",
    "best_name       = best_prediction.name.values[0]\n",
    "best_classifier = best_prediction.classifier.values[0]\n",
    "precision_by_label = scores[best_name][best_classifier][3][0]\n",
    "recall_by_label = scores[best_name][best_classifier][3][1]\n",
    "best_confusion_matrix = scores[best_name][best_classifier][4]\n",
    "\n",
    "# show a side-by-side barchart of precision and recall for each label\n",
    "print(\"\\n\\nPrecision and Recall by Label for classifier \")\n",
    "print(\"Classifier:\", best_classifier, \"Feature set:\", best_name)\n",
    "show_precision_recall_by_label(precision_by_label, recall_by_label,\n",
    "                               best_name, best_classifier, label_encoder)\n",
    "                                                      \n",
    "                                                      \n",
    "# show the confusion matrix for the best performing classifier/feature set\n",
    "show_confusion_matrix(best_confusion_matrix, label_encoder)                                                      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out the confusion matrix and precision/recall by label to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-088f8bf7606e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_confusion_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_confusion_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_precision_by_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_by_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_recall_by_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall_by_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nWriting metrics ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "df_confusion_matrix = pd.DataFrame(best_confusion_matrix)\n",
    "df_precision_by_label = pd.DataFrame(precision_by_label)\n",
    "df_recall_by_label = pd.DataFrame(recall_by_label)\n",
    "\n",
    "print(\"\\nWriting metrics ...\")\n",
    "df_confusion_matrix.to_csv(\"./data/metrics_confusion_matrix.csv\")\n",
    "print(\"done.\")\n",
    "\n",
    "print(\"\\nWriting metrics ...\")\n",
    "df_precision_by_label.to_csv(\"./data/metrics_precision_by_label.csv\")\n",
    "print(\"done.\")\n",
    "\n",
    "print(\"\\nWriting metrics ...\")\n",
    "df_recall_by_label.to_csv(\"./data/metrics_recall_by_label.csv\")\n",
    "print(\"done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
