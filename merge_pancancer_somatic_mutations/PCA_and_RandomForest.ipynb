{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from IPython.display import display\n",
    "from collections import OrderedDict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn import metrics\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_data = pd.read_csv('../data/patient_clinical_data.txt', delimiter=\"\\t\",encoding='iso-8859-1')\n",
    "\n",
    "pcdf_train = pd.read_csv('../data/somatic_mutations_train.csv')\n",
    "pcdf_test = pd.read_csv('../data/somatic_mutations_test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_data = patient_data[['bcr_patient_barcode', 'gender', 'age_at_initial_pathologic_diagnosis', 'race']]\n",
    "patient_data.age_at_initial_pathologic_diagnosis[patient_data.age_at_initial_pathologic_diagnosis == '[Not Available]'] = np.NaN\n",
    "patient_data.race[patient_data.race == '[Not Available]'] = 'NA'\n",
    "patient_data.race[patient_data.race == '[Not Evaluated]'] = 'NA'\n",
    "patient_data.race[patient_data.race == '[Unknown]'] = 'NA'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "ge = preprocessing.LabelEncoder()\n",
    "re = preprocessing.LabelEncoder()\n",
    "\n",
    "\n",
    "patient_data = patient_data[['bcr_patient_barcode', 'gender', 'age_at_initial_pathologic_diagnosis', 'race']]\n",
    "patient_data.gender = ge.fit_transform(patient_data.gender)\n",
    "\n",
    "patient_data.race = ge.fit_transform(patient_data.race)\n",
    "patient_data.age_at_initial_pathologic_diagnosis[patient_data.age_at_initial_pathologic_diagnosis == '[Not Available]'] = np.NaN\n",
    "patient_data.age_at_initial_pathologic_diagnosis = patient_data.age_at_initial_pathologic_diagnosis.astype(float)\n",
    "patient_data = patient_data.fillna(-1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = []\n",
    "\n",
    "for dataset in [pcdf_train, pcdf_test]:\n",
    "  # get column list from training set\n",
    "  cols = list(dataset.columns)\n",
    "  cols[0] = 'Seq'\n",
    "  dataset.columns=cols\n",
    "\n",
    "  # create train and test sets\n",
    "  lmap = pd.Series(dataset.cancer_type.values,index=dataset.patient_barcode).to_dict()\n",
    "\n",
    "  by_patient = pd.pivot_table(dataset, index=\"patient_barcode\", columns='gene',\n",
    "                      values=\"Seq\", aggfunc=\"count\")\n",
    "\n",
    "  # this block below changes the columns to a 1/0 depending on whether the gene exists.\n",
    "#  for col in by_patient.columns:\n",
    "#    by_patient[col] = np.where(by_patient[col]>0, 1.0, 0)\n",
    "\n",
    "  by_patient = pd.DataFrame(by_patient.to_records())\n",
    "  by_patient['cancer_type'] = by_patient[\"patient_barcode\"].map(lmap)\n",
    "  labels = by_patient['cancer_type']\n",
    "  \n",
    "  # add sex, race, age\n",
    "  by_patient = pd.merge(by_patient, patient_data, left_on='patient_barcode', right_on='bcr_patient_barcode',how='left')\n",
    "  by_patient = by_patient.fillna(0.0)\n",
    "  \n",
    "  mat_d = by_patient.drop(['patient_barcode', 'cancer_type', 'bcr_patient_barcode'], axis=1)\n",
    "\n",
    "  datasets.append({'dataset':mat_d, \"labels\":labels})\n",
    "\n",
    "#ensure that columns in test set matches training set\n",
    "missing_in_train = set(datasets[1]['dataset'].columns) - set(datasets[0]['dataset'].columns)\n",
    "missing_in_test = set(datasets[0]['dataset'].columns) - set(datasets[1]['dataset'].columns)\n",
    "\n",
    "for col in missing_in_train:\n",
    "  datasets[0]['dataset'][col] = 0\n",
    "for col in missing_in_test:\n",
    "  datasets[1]['dataset'][col] = 0\n",
    "\n",
    "#reorder test columns to match train\n",
    "cols = datasets[0]['dataset'].columns.tolist()\n",
    "datasets[1]['dataset'] = datasets[1]['dataset'][cols]\n",
    "\n",
    "train_X = datasets[0]['dataset']\n",
    "train_Y = datasets[0]['labels']\n",
    "\n",
    "test_X = datasets[1]['dataset']\n",
    "test_Y = datasets[1]['labels']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "by_ct = pd.pivot_table(pcdf, index=\"cancer_type\", columns='Hugo_Symbol',\n",
    "                    values=\"Seq\", aggfunc=\"count\")\n",
    "\n",
    "def find_features(df, count):\n",
    "  features = set()\n",
    "  cols = list(df.columns)\n",
    "  for _, prow in df.iterrows():\n",
    "    sorted_counts = np.argsort(prow.values)\n",
    "    idxs = set(sorted_counts[-1*count:])\n",
    "    features.update([c for i, c in enumerate(cols) if i in idxs])\n",
    "  return features\n",
    "\n",
    "features = find_features(by_ct, 100)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "shuffle = np.random.permutation(np.arange(mat_d.shape[0]))\n",
    "#X, Y  = principalComponents[shuffle], labels.iloc[shuffle]\n",
    "X, Y  = mat_d.iloc[shuffle], labels.iloc[shuffle]\n",
    "train_X = X[:6000]\n",
    "train_Y = Y[:6000].values\n",
    "\n",
    "test_X = X[6000:]\n",
    "test_Y = Y[6000:].values\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxs = []\n",
    "for col in train_X.columns:\n",
    "  maxs.append(train_X[col].max())\n",
    "print(sorted(maxs, reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "clf = LogisticRegression(C=100)\n",
    "bestfeatures = SelectFromModel(clf, threshold=1.0)\n",
    "\n",
    "best_f = bestfeatures.fit(train_X,train_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_X.columns))\n",
    "supp = best_f.get_support()\n",
    "print(supp[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = [i for i,v in enumerate(supp) if v]\n",
    "print(len(positive))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sel_labels = [cols[i] for i in best_f.get_support(indices=True)]\n",
    "train_X = train_X[sel_labels]\n",
    "test_X = test_X[sel_labels]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression with L1/L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "\n",
    "clf = clf.fit(train_X,  train_Y)\n",
    "pred_Y = clf.predict(test_X)\n",
    "\n",
    "print(\"Accuracy with all features:\",metrics.f1_score(test_Y, pred_Y, average='micro'))\n",
    "sizes=[]\n",
    "accuracies=[]\n",
    "\n",
    "#for c_val in [0.01, 0.03, 0.05, 0.1, 0.3, 0.5, 1, 10, 100, 1000]:\n",
    "for c_val in [0.01, 0.03, 0.05, 0.1, 0.3, 0.5, 1, 10, 100, 1000]:\n",
    "  log_r = LogisticRegression( penalty=\"l1\",  C=c_val, max_iter=200, tol=.01)\n",
    "  log_r.fit(train_X, train_Y)\n",
    "  non_zero1 = list() # list of words with non zero weights\n",
    "  features = train_X.columns\n",
    "  wts = {feat: wt for wt, feat in zip(log_r.coef_.T, features)}\n",
    "  [non_zero1.append(feat) for feat, wts in wts.items() if np.count_nonzero(wts) != 0]\n",
    "  \n",
    "  log_r = LogisticRegression(C=0.5, max_iter=200, penalty=\"l2\")\n",
    "  log_r.fit(train_X[non_zero1], train_Y)\n",
    "  dev_preict2 = log_r.predict(test_X[non_zero1])\n",
    "  sizes.append(len(non_zero1))\n",
    "  accuracies.append(metrics.f1_score(test_Y, dev_preict2, average='weighted'))\n",
    "  print(\"C = %s, Non zero with L1=%s,  F1-Score with L2 and reduced vocabulary =%s \"%(str(c_val), str(len(non_zero1)), \n",
    "                                             metrics.f1_score(test_Y, dev_preict2, average='weighted')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(train_X)\n",
    "ev = np.cumsum(pca.explained_variance_ratio_)\n",
    "evcount = len(ev[ev<99.0])\n",
    "print(\"Number of features that expain 99% of the variance\", evcount)\n",
    "\n",
    "pca = PCA(n_components=evcount)\n",
    "pca.fit(train_X)\n",
    "train_data = pca.transform(train_X)\n",
    "test_data = pca.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "\n",
    "clf = clf.fit(train_data,  train_Y)\n",
    "pred_Y = clf.predict(test_data)\n",
    "\n",
    "print(\"Accuracy:\",metrics.f1_score(test_Y, pred_Y, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=1000, n_jobs=50)\n",
    "\n",
    "clf = clf.fit(train_X, train_Y)\n",
    "pred_Y = clf.predict(test_X)\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.f1_score(test_Y, pred_Y, average='micro')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=1000, n_jobs=20)\n",
    "\n",
    "clf = clf.fit(train_data, train_Y)\n",
    "pred_Y = clf.predict(test_data)\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.f1_score(test_Y, pred_Y, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction using random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=1000, n_jobs=50)\n",
    "\n",
    "sel = SelectFromModel(rf)\n",
    "sel.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open(\"rf_selector\", \"wb\") \n",
    "s = pickle.dumps(sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_feats =  sel.get_support(indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_labels = [cols[i] for i in sel_feats]\n",
    "train_X[sel_labels].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_r_class = []\n",
    "accuracies = []\n",
    "\n",
    "\n",
    "for c_val in [0.01, 0.03, 0.05, 0.1, 0.3, 0.5, 1, 10, 100, 1000]:\n",
    "  log_r = LogisticRegression( C=c_val)\n",
    "  log_r.fit(train_X[sel_labels], train_Y)\n",
    "  log_r_class.append(log_r)\n",
    "  dev_preict2 = log_r.predict(test_X[sel_labels])\n",
    "  accuracies.append(metrics.f1_score(test_Y, dev_preict2, average='weighted'))\n",
    "  print(\"C = %s, accuracy=%s \"%(str(c_val), metrics.f1_score(test_Y, dev_preict2, average='weighted')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_r_class[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=1000, n_jobs=50, max_features=75)\n",
    "\n",
    "clf = clf.fit(train_X[sel_labels], train_Y)\n",
    "pred_Y = clf.predict(test_X[sel_labels])\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.f1_score(test_Y, pred_Y, average='micro')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=1000, n_jobs=50, max_features=75)\n",
    "lrc = LogisticRegression()\n",
    "voting_clf = VotingClassifier(\n",
    "        estimators=[('lr', lrc), ('rf', rfc)],\n",
    "        voting='soft')\n",
    "vc = voting_clf.fit(train_X[sel_labels], train_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_Y = vc.predict(test_X[sel_labels])\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.f1_score(test_Y, pred_Y, average='micro')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision:\",metrics.precision_score(test_Y, pred_Y, average='micro'), 'Recall',\n",
    "      metrics.recall_score(test_Y, pred_Y, average='micro'), \n",
    "      \"F1-score\", metrics.f1_score(test_Y, pred_Y, average='micro'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(test_Y, pred_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
