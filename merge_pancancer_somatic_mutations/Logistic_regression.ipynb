{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('always')\n",
    "\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from textwrap import wrap\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TCGA dictionary information\n",
    "tcga_dict = open(\"tcga_dictionaries.txt\",\"r\")\n",
    "dict_name_index = 0 #Set dictionary index counter to 0\n",
    "for line in tcga_dict:\n",
    "    if line.startswith(\"#\"): #If line starts with #, the next line will be a known dictionary\n",
    "        dict_name_index += 1\n",
    "    elif dict_name_index == 5:\n",
    "        code_to_disease = eval(line)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataAndLabels(cases):\n",
    "    labels_string = cases.cancer_type\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    labels = le.fit_transform(labels_string)\n",
    "\n",
    "    # Get rid of the cancer type and patient_id columns \n",
    "    data = cases[cases.columns[3:]]\n",
    "    return {'data': data, 'labels': labels , 'label_encoder': le }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading case data ...')\n",
    "cases_100 = pd.read_csv(\"pancancer_case_features_100.csv\")\n",
    "cases_250 = pd.read_csv(\"pancancer_case_features_250.csv\")\n",
    "cases_500 = pd.read_csv(\"pancancer_case_features_500.csv\")\n",
    "cases_800  = pd.read_csv(\"pancancer_case_features_800.csv\")\n",
    "cases_1500 = pd.read_csv(\"pancancer_case_features_800.csv\")\n",
    "all_data = {\n",
    "    '100': getDataAndLabels(cases_100),\n",
    "    '250': getDataAndLabels(cases_250),\n",
    "    '500': getDataAndLabels(cases_500),\n",
    "    '800': getDataAndLabels(cases_800),\n",
    "    '1500': getDataAndLabels(cases_800)\n",
    "}\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foldData(data, labels):\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    folds = []\n",
    "    for train_index, dev_index in skf.split(data, labels):\n",
    "        train_data, dev_data     = data.values[train_index], data.values[dev_index]\n",
    "        train_labels, dev_labels = labels[train_index], labels[dev_index]        \n",
    "        folds.append( {'train_data': train_data, 'train_labels': train_labels, \n",
    "                        'dev_data':   dev_data,   'dev_labels': dev_labels })\n",
    "    return folds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitData(data, labels):\n",
    "    train_data_all, test_data, train_labels_all, test_labels = train_test_split(data, labels,\n",
    "                                                               stratify=labels, \n",
    "                                                               test_size=0.25)\n",
    "\n",
    "    train_data, dev_data, train_labels, dev_labels = train_test_split(train_data_all, train_labels_all,\n",
    "                                                                     stratify=train_labels_all, \n",
    "                                                                     test_size=0.20)\n",
    "\n",
    "    print(\"training data:\", train_data.shape)\n",
    "    print(\"dev data     :\", dev_data.shape)\n",
    "    print(\"test data    :\",  test_data.shape)\n",
    "    return {'train_data': train_data, 'train_labels': train_labels, \n",
    "            'dev_data':   dev_data,   'dev_labels': dev_labels,\n",
    "            'test_data':  test_data,  'test_labels': test_labels}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestParams(train_data, train_labels):\n",
    "    mini_train_data, mini_test_data, mini_train_labels, mini_test_labels = train_test_split(train_data, train_labels,\n",
    "                                        stratify=train_labels, \n",
    "                                        test_size=0.55)\n",
    "    \n",
    "    #\n",
    "    # Logistic Regression\n",
    "    #\n",
    "    lr = LogisticRegression(penalty='l2', multi_class = 'ovr', solver='liblinear', max_iter=150)\n",
    "    params = {'C': [0.001, 0.01, 0.1, 0.5, 1, 10]}\n",
    "    logit = GridSearchCV(lr, params, cv=5,\n",
    "                         scoring='accuracy', return_train_score=True)\n",
    "\n",
    "    # Fit  training data\n",
    "    logit.fit(mini_train_data, mini_train_labels)  \n",
    "    # Show the best C parameter to use and the expected accuracy\n",
    "    print('\\nLogistic Regression Classifier, L2 regularization')\n",
    "    print(' Best param:', logit.best_params_)\n",
    "    print(' Accuracy:  ', np.round(logit.best_score_, 4) )\n",
    "    \n",
    "    return logit.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestParamsSVM(train_data, train_labels):\n",
    "    mini_train_data, mini_test_data, mini_train_labels, mini_test_labels = train_test_split(train_data, train_labels,\n",
    "                                        stratify=train_labels, \n",
    "                                        test_size=0.55)\n",
    "    \n",
    "    #\n",
    "    # SVM\n",
    "    #\n",
    "    classifier = LinearSVC(penalty='l2')\n",
    "\n",
    "    params = {'C': [0.001, 0.01, 0.1, 0.5, 1, 10]}\n",
    "    svm = GridSearchCV(classifier, params, cv=4, \n",
    "                       scoring='accuracy', return_train_score=True)\n",
    "\n",
    "    # Fit  training data\n",
    "    svm.fit(mini_train_data, mini_train_labels)  \n",
    "    # Show the best C parameter to use and the expected accuracy\n",
    "    print('\\nSVM Classifier')\n",
    "    print(' Best param:', svm.best_params_)\n",
    "    print(' Accuracy:  ', np.round(svm.best_score_, 4) )\n",
    "    \n",
    "    return svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  Find best C param for Logistic Regression L2\n",
    "#\n",
    "data   = all_data['500']['data']\n",
    "labels = all_data['500']['labels']\n",
    "splits = splitData(data, labels)\n",
    "logit_best_params = getBestParams(splits['train_data'], splits['train_labels'])\n",
    "\n",
    "#\n",
    "#  Find best C param for Linear SVM\n",
    "#\n",
    "svm_best_params = getBestParamsSVM(splits['train_data'], splits['train_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coords_of_max(theArray, n):\n",
    "    # Flatten the 2D array\n",
    "    flat = theArray.flatten()\n",
    "    # Partition so that the we know the sort order for\n",
    "    # the cells with the highest values.  We just\n",
    "    # care about the top n highest values.  So for example,\n",
    "    # if n = 3, get return 3 indices.  \n",
    "    indices = np.argpartition(flat, -n)[-n:]\n",
    "    # Reverse so that we show index of highest value first\n",
    "    # (descending)\n",
    "    indices = indices[np.argsort(-flat[indices])]\n",
    "    # Now return the coordinates for these indices\n",
    "    # for a 2D array.  This will return 2 arrays,\n",
    "    # the first for the row index, the second for the\n",
    "    # column index.  The row index represents the\n",
    "    # actual digit, the column index represents\n",
    "    # the confused digit\n",
    "    return np.unravel_index(indices, theArray.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotMetrics(precision_l1, recall_l1, \n",
    "                accuracy, precision, recall, \n",
    "                precision_by_label, recall_by_label, \n",
    "                svm_accuracy, svm_precision, svm_recall, \n",
    "                svm_precision_by_label, svm_recall_by_label, \n",
    "                confusion, feature_size, label_encoder):\n",
    "    labels = [ '\\n'.join(wrap(l, 8)) for l in feature_size ]       \n",
    "    \n",
    "    plt.rcParams[\"figure.figsize\"] = (20,6)\n",
    "\n",
    "    plt.plot(labels, precision, color='olivedrab', \n",
    "             linewidth=3, label='Precision (Logistic regression) ', marker='o' )\n",
    "    plt.plot(labels, recall, color='olivedrab', linestyle='dashed',\n",
    "             linewidth=3, label='Recall (Logistic regression)', marker='o' )\n",
    "    #plt.plot(labels, accuracy, color='darkolivegreen', linestyle=':',\n",
    "    #         linewidth=3, label='Accuracy (Logistic regression)', marker='v' )\n",
    "\n",
    "    plt.plot(labels, svm_precision, color='slateblue', \n",
    "             linewidth=3, label='Precision (SVM)', marker='o' )\n",
    "    plt.plot(labels, svm_recall, color='slateblue', linestyle='dashed',\n",
    "             linewidth=3, label='Recall (SVM)', marker='o' )\n",
    "    #plt.plot(labels, svm_accuracy, color='darkslateblue', linestyle=':',\n",
    "    #         linewidth=3, label='Accuracy (SVM)', marker='v' )\n",
    "\n",
    "    #plt.plot(labels, precision_l1, color='gray',alpha=.4,\n",
    "    #         linewidth=3, label='L1 precision at different C values', marker='o' )\n",
    "    #plt.plot(labels, recall_l1, color='gray', linestyle='dashed',alpha=.6,\n",
    "    #         linewidth=3, label='L1 at different C values', marker='o' )\n",
    "    \n",
    "    \n",
    "    plt.yticks(np.arange(.42, .6, .01))\n",
    "    plt.ylabel('Precision, Recall', fontsize=20)\n",
    "    plt.xlabel('Feature size with L1 regularization at different C parameters', fontsize=20, labelpad=20)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    # find optimal f1\n",
    "    best_idx = np.argmax(precision)\n",
    "    \n",
    "    # Show precision and recall across different labels\n",
    "    showPrecisionRecallPairByLabel(precision_by_label[best_idx], recall_by_label[best_idx], label_encoder,\n",
    "                                  'Logistic Regression', ['olivedrab', 'darkolivegreen'])\n",
    "    showPrecisionRecallPairByLabel(svm_precision_by_label[best_idx], svm_recall_by_label[best_idx], label_encoder,\n",
    "                                  'SVM', ['slateblue', 'darkslateblue'])\n",
    "    \n",
    "    \n",
    "    # Get the confusion matrix for the optimal precision\n",
    "    # Show the labels that have the highest error rate\n",
    "    conf_mx = confusion[best_idx]\n",
    "    showTopConfused(conf_mx, label_encoder)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showTopConfused(conf_mx, label_encoder):\n",
    "    # Determine the error rates for each misclassification pair\n",
    "    row_sums = conf_mx.sum(axis=1, keepdims=True)\n",
    "    norm_conf_mx = conf_mx / row_sums\n",
    "    # Set the error rates for correctly classified pairs (the diagonal) to zero\n",
    "    np.fill_diagonal(norm_conf_mx, 0)\n",
    "    \n",
    "    max_coords = coords_of_max(norm_conf_mx, 20)\n",
    "    confusion_rows = []\n",
    "    for i in range(len(max_coords[0])):\n",
    "\n",
    "        # This is the actual label\n",
    "        actual_label_idx  = max_coords[0][i]\n",
    "        actual_label      = label_encoder.inverse_transform([actual_label_idx])[0]\n",
    "\n",
    "        # This is the predicted label\n",
    "        predicted_label_idx = max_coords[1][i]\n",
    "        predicted_label = label_encoder.inverse_transform([predicted_label_idx])[0]\n",
    "        \n",
    "        # This is the error rate\n",
    "        error_rate  = norm_conf_mx[max_coords[0][i], max_coords[1][i]]\n",
    "        error_count = conf_mx[max_coords[0][i], max_coords[1][i]]\n",
    "\n",
    "        row = list([ actual_label,                     \n",
    "                     code_to_disease[actual_label][0], \n",
    "                     predicted_label,\n",
    "                     code_to_disease[predicted_label][0], \n",
    "                     error_rate, \n",
    "                     error_count ])\n",
    "        confusion_rows.append(row)\n",
    "    \n",
    "    df = pd.DataFrame(confusion_rows, columns=['actual', 'predicted',  'actual_name', 'predicted_name', 'error_rate', 'error_count'])\n",
    "    display(df)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showPrecisionRecallPairByLabel(precision_by_label, recall_by_label, label_encoder, classifier_name, colors):\n",
    "    labels = []\n",
    "    for i in range(len(precision_by_label)):\n",
    "        label = label_encoder.inverse_transform([i])[0]\n",
    "        labels.append(label)\n",
    "    \n",
    "    y_pos = np.arange(len(labels))    \n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=False)\n",
    "\n",
    "    ax1.invert_xaxis()\n",
    "    ax1.yaxis.tick_right()\n",
    "    \n",
    "    ax1.set_yticks(y_pos)\n",
    "    ax1.set_yticklabels(labels)\n",
    "    \n",
    "    ax2.set_yticks(y_pos)\n",
    "    ax2.set_yticklabels(labels)\n",
    "        \n",
    "    ax1.barh(y_pos, precision_by_label, color=colors[0] , label=\"precision\")\n",
    "    ax2.barh(y_pos, recall_by_label,    color=colors[1],  label='recall')\n",
    "\n",
    "    ax1.set_title('Precision( ' + classifier_name + ')')\n",
    "    ax2.set_title('Recall (' + classifier_name + ')')\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runLogitL1(train_data, train_labels, dev_data, dev_labels, c_param):\n",
    "    l1 = LogisticRegression(penalty='l1', tol=.01, \n",
    "                            solver=\"liblinear\", multi_class=\"ovr\",\n",
    "                            max_iter=500, C=c_param)\n",
    "    # Fit model\n",
    "    l1.fit(train_data, train_labels) \n",
    "    # Predict\n",
    "    predict = l1.predict(dev_data)\n",
    "    # Get precision, recall, f1 scores\n",
    "    scores = precision_recall_fscore_support(dev_labels, predict, average='weighted')  \n",
    "    \n",
    "    # Get the features with non-zero coefficients.  We will use\n",
    "    # this list to reduce the features for the\n",
    "    # following logistic regression with L2 regularization\n",
    "    non_zero_sums = np.where(np.sum(l1.coef_, axis=0) != 0)\n",
    "    names = np.array(list(train_data.columns))\n",
    "    non_zero_names = names[non_zero_sums] \n",
    "    \n",
    "    return {'scores': scores, 'non_zero_genes': non_zero_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminateFeatures(train_data, train_labels, dev_data, dev_labels, logit_best_params, label_encoder):\n",
    "\n",
    "    params = {'C':  [1000, 100, 10, 1, .5, .3, .1, .05]}\n",
    "\n",
    "\n",
    "    # Now perform logistic regression on this training set with reduced features\n",
    "    # as well as the orginal non-reduced training set.  Run over different\n",
    "    # C values to plot differences in accuracy\n",
    "    precision_l1        = []\n",
    "    recall_l1           = []\n",
    "    \n",
    "    accuracy            = []\n",
    "    precision           = []\n",
    "    recall              = []\n",
    "    precision_by_label  = []\n",
    "    recall_by_label     = []\n",
    "    \n",
    "    svm_accuracy            = []\n",
    "    svm_precision           = []\n",
    "    svm_recall              = []\n",
    "    svm_precision_by_label  = []\n",
    "    svm_recall_by_label     = []\n",
    "\n",
    "    feature_size        = []\n",
    "    confusion           = []\n",
    "\n",
    "\n",
    "    for c_param in reversed(params['C']):\n",
    "        # Keep this random seed here to make comparison easier.\n",
    "        np.random.seed(0)\n",
    "\n",
    "        #\n",
    "        # Perform Logistic Regression on different C values\n",
    "        # using L1 regularization\n",
    "        #\n",
    "        l1_info = runLogitL1(train_data, train_labels, dev_data, dev_labels, c_param)    \n",
    "        non_zero_genes = l1_info['non_zero_genes']\n",
    "        feature_size.append(str(len(non_zero_genes)) + ' (C=' + str(c_param) + \")\")\n",
    "        precision_l1.append(l1_info['scores'][0])\n",
    "        recall_l1.append(l1_info['scores'][1])\n",
    "\n",
    "\n",
    "        #\n",
    "        # Reduce feature size, only keeping features with non-zero weights \n",
    "        # found using l1 regularization\n",
    "        #\n",
    "        min_train_data = train_data[non_zero_genes]\n",
    "        min_dev_data   = dev_data[non_zero_genes]\n",
    "\n",
    "\n",
    "        # Run logistic regression with L2 regularization on reduced\n",
    "        # feature set\n",
    "        lr = LogisticRegression(penalty='l2', tol=.01, max_iter=150, \n",
    "                                C=logit_best_params['C'], solver=\"liblinear\", multi_class=\"ovr\")\n",
    "        lr.fit(min_train_data, train_labels) \n",
    "        predict = lr.predict(min_dev_data)\n",
    "\n",
    "        # Get precision, recall, f1 scores\n",
    "        the_accuracy = accuracy_score(dev_labels, predict)\n",
    "        scores = precision_recall_fscore_support(dev_labels, predict, average='weighted')\n",
    "        scores_by_label = precision_recall_fscore_support(dev_labels, predict, average=None)\n",
    "\n",
    "        # Get confusion matrix\n",
    "        confusion_mx = confusion_matrix(dev_labels, predict)\n",
    "\n",
    "        accuracy.append(the_accuracy)\n",
    "        precision.append(scores[0])\n",
    "        recall.append(scores[1])\n",
    "        precision_by_label.append(scores_by_label[0])\n",
    "        recall_by_label.append(scores_by_label[1])\n",
    "        confusion.append(confusion_mx)\n",
    "        \n",
    "        #\n",
    "        # Run Linear SVM\n",
    "        #\n",
    "        svm = LinearSVC(penalty='l2', C=svm_best_params['C'])\n",
    "\n",
    "        svm.fit(min_train_data, train_labels,) \n",
    "        predict = svm.predict(min_dev_data)\n",
    "\n",
    "        # Get precision, recall, f1 scores\n",
    "        svm_accuracy_score = accuracy_score(dev_labels, predict)\n",
    "        svm_scores = precision_recall_fscore_support(dev_labels, predict, average='weighted')\n",
    "        svm_scores_by_label = precision_recall_fscore_support(dev_labels, predict, average=None)\n",
    "\n",
    "        svm_accuracy.append(svm_accuracy_score)\n",
    "        svm_precision.append(svm_scores[0])\n",
    "        svm_recall.append(svm_scores[1])\n",
    "        svm_precision_by_label.append(svm_scores_by_label[0])\n",
    "        svm_recall_by_label.append(svm_scores_by_label[1])\n",
    "        \n",
    "    plotMetrics(precision_l1, recall_l1, \n",
    "                accuracy, precision, recall, \n",
    "                precision_by_label, recall_by_label,\n",
    "                svm_accuracy, svm_precision, svm_recall, \n",
    "                svm_precision_by_label, svm_recall_by_label,\n",
    "                confusion, feature_size, label_encoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#  Run classifier on each binary matrix, that has different\n",
    "#  number of columns (genes).  Iterate through different\n",
    "#  C values, using Logistic Regression L1 regularization \n",
    "#  to eliminate features.  Now run Logistic Regression, L2\n",
    "#  regularization and keep track of precision, recall,\n",
    "#  and confusion matrix.  Plot these metrics per feature\n",
    "#  size and show the confusion matrix for the best performing\n",
    "#  feature size.\n",
    "#\n",
    "for top_n_genes, data_object in all_data.items():\n",
    "    data          = data_object['data']\n",
    "    labels        = data_object['labels']\n",
    "    label_encoder = data_object['label_encoder']\n",
    "    splits = splitData(data, labels)\n",
    "    eliminateFeatures(splits['train_data'], splits['train_labels'],\n",
    "                      splits['dev_data'], splits['dev_labels'], logit_best_params, label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
