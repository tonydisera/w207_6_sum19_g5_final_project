{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading case data ...')\n",
    "cases = pd.read_csv(\"pancancer_case_features.csv\")\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_string = cases.cancer_type\n",
    "le = preprocessing.LabelEncoder()\n",
    "labels = le.fit_transform(labels_string)\n",
    "\n",
    "\n",
    "# Get rid of the cancer type and patient_id columns \n",
    "data = cases[cases.columns[3:]]\n",
    "\n",
    "display(labels)\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skf = StratifiedKFold(n_splits=10)\n",
    "#for train_index, test_index in skf.split(data, labels):\n",
    "#    train_data, test_data     = data.values[train_index], data.values[test_index]\n",
    "#    train_labels, test_labels = labels[train_index], labels[test_index]\n",
    "#    print(len(train_data), len(test_data))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_all, test_data, train_labels_all, test_labels = train_test_split(data, labels,\n",
    "                                                           stratify=labels, \n",
    "                                                           test_size=0.25)\n",
    "\n",
    "train_data, dev_data, train_labels, dev_labels = train_test_split(train_data_all, train_labels_all,\n",
    "                                                                 stratify=train_labels_all, \n",
    "                                                                 test_size=0.20)\n",
    "\n",
    "print(\"training data:\", train_data.shape)\n",
    "print(\"dev data     :\", dev_data.shape)\n",
    "print(\"test data    :\",  test_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "#\n",
    "# Multinomial Naive Bayes\n",
    "#\n",
    "mnb = MultinomialNB()\n",
    "params = {'alpha': [0.001, 0.01, 0.1, 0.5, 1, 10]}\n",
    "mnb = GridSearchCV(mnb, params, cv=5,\n",
    "                               scoring='accuracy', return_train_score=True)\n",
    "# Fit  training data\n",
    "mnb.fit(train_data, train_labels)  \n",
    "# Show the best alpha parameter to use and the expected accuracy\n",
    "print('\\nMultinomial Naive Bayes Classifier')\n",
    "print(' Best param:',   mnb.best_params_)\n",
    "print(' Accuracy:  ',   np.round(mnb.best_score_, 4) )\n",
    "\n",
    "\n",
    "#\n",
    "# Logistic Regression\n",
    "#\n",
    "lr = LogisticRegression(penalty='l2', multi_class = 'ovr', solver='liblinear', max_iter=150)\n",
    "params = {'C': [0.001, 0.01, 0.1, 0.5, 1, 10]}\n",
    "logit = GridSearchCV(lr, params, cv=5,\n",
    "                     scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# Fit  training data\n",
    "logit.fit(train_data, train_labels)  \n",
    "# Show the best C parameter to use and the expected accuracy\n",
    "print('\\nLogistic Regression Classifier, L2 regularization')\n",
    "print(' Best param:', logit.best_params_)\n",
    "print(' Accuracy:  ', np.round(logit.best_score_, 4) )\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runLogitL1(train_data, train_labels, dev_data, dev_labels, c_param):\n",
    "    l1 = LogisticRegression(penalty='l1', tol=.01, \n",
    "                            solver=\"liblinear\", multi_class=\"ovr\",\n",
    "                            max_iter=500, C=c_param)\n",
    "    # Fit model\n",
    "    l1.fit(train_data, train_labels) \n",
    "    # Predict\n",
    "    predict = l1.predict(dev_data)\n",
    "    # Get precision, recall, f1 scores\n",
    "    scores = precision_recall_fscore_support(dev_labels, predict, average='weighted', labels=np.unique(predict))  \n",
    "    \n",
    "    # Get the features with non-zero coefficients.  We will use\n",
    "    # this list to reduce the features for the\n",
    "    # following logistic regression with L2 regularization\n",
    "    non_zero_sums = np.where(np.sum(l1.coef_, axis=0) != 0)\n",
    "    names = np.array(list(train_data.columns))\n",
    "    non_zero_names = names[non_zero_sums] \n",
    "    \n",
    "    return {'scores': scores, 'non_zero_genes': non_zero_names}\n",
    "\n",
    "from textwrap import wrap\n",
    "params = {'C':  [1000, 100, 10, 1, .5, .3, .1, .05]}\n",
    "\n",
    "\n",
    "# Now perform logistic regression on this training set with reduced features\n",
    "# as well as the orginal non-reduced training set.  Run over different\n",
    "# C values to plot differences in accuracy\n",
    "precision_l1        = []\n",
    "recall_l1           = []\n",
    "precision_l2      = []\n",
    "recall_l2           = []\n",
    "feature_size        = []\n",
    "\n",
    "\n",
    "for c_param in reversed(params['C']):\n",
    "    # Keep this random seed here to make comparison easier.\n",
    "    np.random.seed(0)\n",
    "\n",
    "    #\n",
    "    # Perform Logistic Regression on different C values\n",
    "    # using L1 regularization\n",
    "    #\n",
    "    l1_info = runLogitL1(train_data, train_labels, dev_data, dev_labels, c_param)    \n",
    "    non_zero_genes = l1_info['non_zero_genes']\n",
    "    feature_size.append(str(len(non_zero_genes)) + ' (C=' + str(c_param) + \")\")\n",
    "    precision_l1.append(l1_info['scores'][0])\n",
    "    recall_l1.append(l1_info['scores'][1])\n",
    "\n",
    "\n",
    "    #\n",
    "    # Reduce feature size, only keeping features with non-zero weights \n",
    "    # found using l1 regularization\n",
    "    #\n",
    "    min_train_data = train_data[non_zero_genes]\n",
    "    min_dev_data   = dev_data[non_zero_genes]\n",
    "\n",
    "\n",
    "    # Run logistic regression with L2 regularization on reduced\n",
    "    # feature set\n",
    "    lr = LogisticRegression(penalty='l2', tol=.01, max_iter=150, \n",
    "                            C=logit.best_params_['C'], solver=\"liblinear\", multi_class=\"ovr\")\n",
    "    lr.fit(min_train_data, train_labels) \n",
    "\n",
    "    predict = lr.predict(min_dev_data)\n",
    "    \n",
    "    # Get precision, recall, f1 scores\n",
    "    scores = precision_recall_fscore_support(dev_labels, predict, average='weighted', labels=np.unique(predict))  \n",
    "    precision_l2.append(scores[0])\n",
    "    recall_l2.append(scores[1])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = [ '\\n'.join(wrap(l, 8)) for l in feature_size ]        \n",
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "plt.plot(labels, precision_l1, color='mediumblue',\n",
    "         linewidth=3, label='L1 precision at different C values', marker='o' )\n",
    "plt.plot(labels, recall_l1, color='mediumblue', linestyle='dashed',\n",
    "         linewidth=3, label='L1 at different C values', marker='o' )\n",
    "\n",
    "plt.plot(labels, precision_l2, color='darkorange', \n",
    "         linewidth=3, label='L2 precision with reduced number of features', marker='o' )\n",
    "plt.plot(labels, recall_l2, color='darkorange', linestyle='dashed',\n",
    "         linewidth=3, label='L2 recall with reduced number of features', marker='o' )\n",
    "\n",
    "plt.yticks(np.arange(.34, .8, .01))\n",
    "plt.ylabel('Precision, Recall', fontsize=20)\n",
    "plt.xlabel('Feature size with L1 regularization at different C parameters', fontsize=20, labelpad=20)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
