{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading case data ...')\n",
    "cases = pd.read_csv(\"pancancer_case_features_100.csv\")\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** If we opened the 100 case features file, why does the table below show 1063 columns. Also, I think we should encode the labels, perform PCA and select the number of features in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_string = cases.cancer_type\n",
    "le = preprocessing.LabelEncoder()\n",
    "labels = le.fit_transform(labels_string)\n",
    "\n",
    "# Get rid of the cancer type and patient_id columns \n",
    "data = cases[cases.columns[3:]]\n",
    "\n",
    "display(labels)\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed before, we have a LOT of features. However, we know that most genes are not active for our dataset. Let us perform a dimensionality reduction exercise with our data. **Note:** I am performing this with the entire dataset. However, we should do this only with the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(data)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >= 0.95) + 1\n",
    "print(\"PCA suggested dimension: %d\" %d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skf = StratifiedKFold(n_splits=10)\n",
    "#for train_index, test_index in skf.split(data, labels):\n",
    "#    train_data, test_data     = data.values[train_index], data.values[test_index]\n",
    "#    train_labels, test_labels = labels[train_index], labels[test_index]\n",
    "#    print(len(train_data), len(test_data))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split our data into three categories. First we split the data into *training data* and *test data*. For this, we use the common splitting rule of 70:30. We take care to *stratify* the separation based on the labels because we have seen that some cancer types are extremely sparsely represented. Next, we create a *development* dataset used for tuning the hyperparameters of the various models we plan to try. Here, we use separate **20%** of the testing data for development purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_all, test_data, train_labels_all, test_labels = train_test_split(data, labels,\n",
    "                                                           stratify=labels, \n",
    "                                                           test_size=0.30)\n",
    "\n",
    "train_data, dev_data, train_labels, dev_labels = train_test_split(train_data_all, train_labels_all,\n",
    "                                                                 stratify=train_labels_all, \n",
    "                                                                 test_size=0.20)\n",
    "\n",
    "print(\"training data:\", train_data.shape)\n",
    "print(\"dev data     :\", dev_data.shape)\n",
    "print(\"test data    :\",  test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes ##\n",
    "\n",
    "First, we try the NaiveBayes classifier. NaiveBayes is a generative model that works by using the training data to create a probability distribution of the different features for the different labels. For this, it simply uses the frequency of the feature conditional on the label from the training dataset. It then uses the independence assumption of these features conditional on the label in the Bayes rule to compute the probability of a given sample in the test dataset. Because of the assumption of independence (which may be questionable for cancer data, even conditional on the label), the probability calculation is simply the normalized product of learnt feature probabilities for each label. The classifier then simply predicts the label with the highest computed probability.\n",
    "\n",
    "Since some features may be completely absent in the training dataset for some labels, we use Laplace smoothing to ensure that the probability product is not zero. This is represented by the hyperparameter **alpha** of the classifier. We use *GridSearchCV* using a cross-validation size of **5** to find the most optimal value for this parameter.\n",
    "\n",
    "**Question:** If we are using GridSearchCV, should we still create a dev dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Multinomial Naive Bayes\n",
    "#\n",
    "mnb = MultinomialNB()\n",
    "params = {'alpha': [0.001, 0.01, 0.1, 0.5, 1, 10]}\n",
    "mnb = GridSearchCV(mnb, params, cv=5,\n",
    "                   scoring='accuracy', return_train_score=True)\n",
    "# Fit  training data\n",
    "mnb.fit(train_data, train_labels)  \n",
    "mnb_best_alpha = mnb.best_params_\n",
    "mnb_cv_accuracy = np.round(mnb.best_score_, 4)\n",
    "\n",
    "# Show the best alpha parameter to use and the expected accuracy\n",
    "print('\\nMultinomial Naive Bayes Classifier')\n",
    "print(' Best param (alpha):\\t',   mnb_best_alpha)\n",
    "print(' Accuracy (CV): \\t',   mnb_cv_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearch reports back an optimial alpha of 1 and a CV accuracy on the training set of **44%**. We now test these parameters on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb = MultinomialNB(1)\n",
    "mnb.fit(train_data, train_labels)\n",
    "mnb_pred = mnb.predict(test_data)\n",
    "\n",
    "#plt.plot(fpr, tpr, \"b:\", label=\"SGD\")\n",
    "#plot_roc_curve(fpr_forest, tpr_forest, \"Random Forest\")\n",
    "#plt.legend(loc=\"lower right\")\n",
    "#plt.show()\n",
    "print(\"Test set accuracy:\\t\", metrics.accuracy_score(test_labels, mnb_pred))\n",
    "print(\"\\n\")\n",
    "print(metrics.classification_report(test_labels, mnb_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Logistic Regression\n",
    "#\n",
    "lr = LogisticRegression(penalty='l2', multi_class = 'ovr', solver='liblinear', max_iter=150)\n",
    "params = {'C': [0.001, 0.01, 0.1, 0.5, 1, 10]}\n",
    "logit = GridSearchCV(lr, params, cv=5,\n",
    "                     scoring='accuracy', return_train_score=True)\n",
    "\n",
    "# Fit  training data\n",
    "logit.fit(train_data, train_labels)  \n",
    "# Show the best C parameter to use and the expected accuracy\n",
    "print('\\nLogistic Regression Classifier, L2 regularization')\n",
    "print(' Best param:', logit.best_params_)\n",
    "print(' Accuracy:  ', np.round(logit.best_score_, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runLogitL1(train_data, train_labels, dev_data, dev_labels, c_param):\n",
    "    l1 = LogisticRegression(penalty='l1', tol=.01, \n",
    "                            solver=\"liblinear\", multi_class=\"ovr\",\n",
    "                            max_iter=500, C=c_param)\n",
    "    # Fit model\n",
    "    l1.fit(train_data, train_labels) \n",
    "    # Predict\n",
    "    predict = l1.predict(dev_data)\n",
    "    # Get precision, recall, f1 scores\n",
    "    scores = precision_recall_fscore_support(dev_labels, predict, average='weighted', labels=np.unique(predict))  \n",
    "    \n",
    "    # Get the features with non-zero coefficients.  We will use\n",
    "    # this list to reduce the features for the\n",
    "    # following logistic regression with L2 regularization\n",
    "    non_zero_sums = np.where(np.sum(l1.coef_, axis=0) != 0)\n",
    "    names = np.array(list(train_data.columns))\n",
    "    non_zero_names = names[non_zero_sums] \n",
    "    \n",
    "    return {'scores': scores, 'non_zero_genes': non_zero_names}\n",
    "\n",
    "from textwrap import wrap\n",
    "params = {'C':  [1000, 100, 10, 1, .5, .3, .1, .05]}\n",
    "\n",
    "\n",
    "# Now perform logistic regression on this training set with reduced features\n",
    "# as well as the orginal non-reduced training set.  Run over different\n",
    "# C values to plot differences in accuracy\n",
    "precision_l1        = []\n",
    "recall_l1           = []\n",
    "precision_l2      = []\n",
    "recall_l2           = []\n",
    "feature_size        = []\n",
    "\n",
    "\n",
    "for c_param in reversed(params['C']):\n",
    "    # Keep this random seed here to make comparison easier.\n",
    "    np.random.seed(0)\n",
    "\n",
    "    #\n",
    "    # Perform Logistic Regression on different C values\n",
    "    # using L1 regularization\n",
    "    #\n",
    "    l1_info = runLogitL1(train_data, train_labels, dev_data, dev_labels, c_param)    \n",
    "    non_zero_genes = l1_info['non_zero_genes']\n",
    "    feature_size.append(str(len(non_zero_genes)) + ' (C=' + str(c_param) + \")\")\n",
    "    precision_l1.append(l1_info['scores'][0])\n",
    "    recall_l1.append(l1_info['scores'][1])\n",
    "\n",
    "\n",
    "    #\n",
    "    # Reduce feature size, only keeping features with non-zero weights \n",
    "    # found using l1 regularization\n",
    "    #\n",
    "    min_train_data = train_data[non_zero_genes]\n",
    "    min_dev_data   = dev_data[non_zero_genes]\n",
    "\n",
    "\n",
    "    # Run logistic regression with L2 regularization on reduced\n",
    "    # feature set\n",
    "    lr = LogisticRegression(penalty='l2', tol=.01, max_iter=150, \n",
    "                            C=logit.best_params_['C'], solver=\"liblinear\", multi_class=\"ovr\")\n",
    "    lr.fit(min_train_data, train_labels) \n",
    "\n",
    "    predict = lr.predict(min_dev_data)\n",
    "    \n",
    "    # Get precision, recall, f1 scores\n",
    "    scores = precision_recall_fscore_support(dev_labels, predict, average='weighted', labels=np.unique(predict))  \n",
    "    precision_l2.append(scores[0])\n",
    "    recall_l2.append(scores[1])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = [ '\\n'.join(wrap(l, 8)) for l in feature_size ]        \n",
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "plt.plot(labels, precision_l1, color='mediumblue',\n",
    "         linewidth=3, label='L1 precision at different C values', marker='o' )\n",
    "plt.plot(labels, recall_l1, color='mediumblue', linestyle='dashed',\n",
    "         linewidth=3, label='L1 at different C values', marker='o' )\n",
    "\n",
    "plt.plot(labels, precision_l2, color='darkorange', \n",
    "         linewidth=3, label='L2 precision with reduced number of features', marker='o' )\n",
    "plt.plot(labels, recall_l2, color='darkorange', linestyle='dashed',\n",
    "         linewidth=3, label='L2 recall with reduced number of features', marker='o' )\n",
    "\n",
    "plt.yticks(np.arange(.34, .8, .01))\n",
    "plt.ylabel('Precision, Recall', fontsize=20)\n",
    "plt.xlabel('Feature size with L1 regularization at different C parameters', fontsize=20, labelpad=20)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
