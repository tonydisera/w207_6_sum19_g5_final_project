{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dRLi1FBsby4R"
   },
   "source": [
    "# W207.6 Final Project - Predicting Cancer Type from Tumor Mutations\n",
    "### Tony Di Sera, Vijay Singh, Rajiv Nair, Jeremey Fraenkel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6NrLTq1-bVmt"
   },
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u1NF-T6Jby4U"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "from textwrap import wrap\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import time\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "from tensorflow.keras.layers import Dense as Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "\n",
    "# Establish the colors for each cancer type\n",
    "label_colors = []\n",
    "cm = plt.get_cmap('tab20b')\n",
    "for i in range(20):\n",
    "    label_colors.append(cm(i))\n",
    "cm = plt.get_cmap('tab20c')\n",
    "for i in range(13):\n",
    "    label_colors.append(cm(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "NVVoSJj8by4i",
    "outputId": "78557bfc-1622-4c33-96e3-36e17e0d1a0e"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "BLpMLAMqby4l",
    "outputId": "56444f10-50f0-4c22-dbfc-252569929c4e"
   },
   "outputs": [],
   "source": [
    "#cd /content/drive/My Drive/berkeley/W207 machine learning/Final Project/w207_6_sum19_g5_final_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "cruqTsRrby4e",
    "outputId": "85cfe90d-4189-4801-d5df-84b5ccbc85c2"
   },
   "outputs": [],
   "source": [
    "#if tf.test.gpu_device_name() != '/device:GPU:0':\n",
    "#  print('WARNING: GPU device not found.')\n",
    "#else:\n",
    "#  print('SUCCESS: Found GPU: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O-7lMnfrby4y"
   },
   "outputs": [],
   "source": [
    "# create the directory where the downloaded directory is stored\n",
    "data_dir = \"./data\"\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    \n",
    "# create the directory where the metrics are stored\n",
    "metrics_dir = \"./metrics\"\n",
    "if not os.path.isdir(metrics_dir):\n",
    "    os.makedirs(metrics_dir)\n",
    "    \n",
    "# create the raw where the source data is stored\n",
    "raw_dir = \"./raw\"\n",
    "if not os.path.isdir(raw_dir):\n",
    "    os.makedirs(raw_dir)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OUYcwsHcIelo"
   },
   "outputs": [],
   "source": [
    "# This downloads a dictionary file\n",
    "dictionary_filename = \"./raw/tcga_dictionaries.txt\"\n",
    "if os.path.isfile(dictionary_filename):\n",
    "    print(\"Skipping download, as file %s is present\" %(dictionary_filename))\n",
    "else:\n",
    "    print('Downloading dictionary file...')\n",
    "    url = 'https://w207-final-project.s3.amazonaws.com/raw/tcga_dictionaries.txt'  \n",
    "    urllib.request.urlretrieve(url, dictionary_filename)  \n",
    "print(\"done.\")\n",
    "\n",
    "\n",
    "# This loads the data dictionary to will convert\n",
    "# the tumor_sample_barcode into a cancer_type\n",
    "# and provide full names for the cancer types\n",
    "tcga_dict = open(\"./raw/tcga_dictionaries.txt\",\"r\")\n",
    "dict_name_index = 0 #Set dictionary index counter to 0\n",
    "for line in tcga_dict:\n",
    "    if line.startswith(\"#\"): #If line starts with #, the next line will be a known dictionary\n",
    "        dict_name_index += 1\n",
    "    elif dict_name_index == 4:\n",
    "        tissue_source_site = eval(line)            \n",
    "    elif dict_name_index == 5:\n",
    "        code_to_disease = eval(line)\n",
    "    elif dict_name_index == 6:\n",
    "        disease_to_code = eval(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1jJefDaHby7m"
   },
   "source": [
    "# Run the Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k8UQU7csby7n"
   },
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zK-UScuMa25r"
   },
   "source": [
    "### The data dictionary\n",
    "All data source files are downloaded above.  This dataset, is a data dictionary\n",
    "that will allow us to translate cancer type codes to cancer type names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yf_agA3Ba88f"
   },
   "outputs": [],
   "source": [
    "# This loads the data dictionary to will convert\n",
    "# the tumor_sample_barcode into a cancer_type\n",
    "# and provide full names for the cancer types\n",
    "tcga_dict = open(\"./raw/tcga_dictionaries.txt\",\"r\")\n",
    "dict_name_index = 0 #Set dictionary index counter to 0\n",
    "for line in tcga_dict:\n",
    "    if line.startswith(\"#\"): #If line starts with #, the next line will be a known dictionary\n",
    "        dict_name_index += 1\n",
    "    elif dict_name_index == 4:\n",
    "        tissue_source_site = eval(line)            \n",
    "    elif dict_name_index == 5:\n",
    "        code_to_disease = eval(line)\n",
    "    elif dict_name_index == 6:\n",
    "        disease_to_code = eval(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tvpezaGsby7o"
   },
   "outputs": [],
   "source": [
    "def getDataAndLabels(name, features, label_encoder):\n",
    "    labels_string = features.cancer_type\n",
    "   \n",
    "    labels        = label_encoder.fit_transform(labels_string)\n",
    "\n",
    "    # Get rid of the cancer type and patient_barcode columns \n",
    "    data = features[features.columns[3:]]\n",
    "\n",
    "    return {'name': name, 'feature_size': data.shape[1],\n",
    "            'data': data, 'labels': labels , 'label_encoder': label_encoder }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "u2kc_vGOby7r",
    "outputId": "8fa86854-348d-463d-97e7-e4fed4ae3811"
   },
   "outputs": [],
   "source": [
    "print('Loading training data ...')\n",
    "\n",
    "filepath = \"./data/features_*\"\n",
    "\n",
    "# label encoder\n",
    "label_encoder   = preprocessing.LabelEncoder()\n",
    "\n",
    "# get all file names for the feature datasets\n",
    "train_files = glob.glob(filepath + \".train.csv\")\n",
    "all_train_data = {}\n",
    "\n",
    "# load all of the files\n",
    "for filename in train_files:\n",
    "    \n",
    "    name = filename[16:-10]\n",
    "    print(\" \", name)\n",
    "    train_features = pd.read_csv(filename)\n",
    "    all_train_data[name] = getDataAndLabels(name, train_features, label_encoder)\n",
    "\n",
    "print(\"done.\")\n",
    "\n",
    "\n",
    "print('Loading test data ...')\n",
    "\n",
    "test_files = glob.glob(filepath + \".test.csv\")\n",
    "all_test_data = {}\n",
    "for filename in test_files:\n",
    "    \n",
    "    name = filename[16:-9]\n",
    "    print(\" \", name)\n",
    "    test_features = pd.read_csv(filename)\n",
    "    all_test_data[name] = getDataAndLabels(name, test_features, label_encoder)\n",
    "\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y_50vHGONZwI"
   },
   "source": [
    "## Functions for tracking metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KZsxQONYby8K"
   },
   "outputs": [],
   "source": [
    "def get_saved_metrics():\n",
    "    metrics_filename = \"./metrics/metrics.csv\"\n",
    "    if os.path.isfile(metrics_filename):\n",
    "        metrics_df = pd.read_csv(metrics_filename)\n",
    "        metrics =  [row for row in metrics_df.T.to_dict().values()]\n",
    "        return metrics\n",
    "    else:\n",
    "        return []\n",
    "      \n",
    "def get_saved_metrics_dataframe():\n",
    "    metrics_filename = \"./metrics/metrics.csv\"\n",
    "    if os.path.isfile(metrics_filename):\n",
    "        metrics_df = pd.read_csv(metrics_filename)\n",
    "        return metrics_df\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def save_metrics(name, classifier, metrics, prf_by_label, confusion_mx):\n",
    "    \n",
    "    metrics_df = pd.DataFrame(metrics, columns=['name', 'classifier', 'feature_size', \n",
    "                                                'accuracy', 'precision', 'recall', 'f1', \n",
    "                                                'time'])\n",
    "    \n",
    "    # Write out scores as csv files\n",
    "    metrics_df.to_csv(\"./metrics/metrics.csv\")\n",
    "    \n",
    "    # Write out confusion matrix to csv file    \n",
    "    confusion_mx_df = pd.DataFrame.from_dict(confusion_mx)\n",
    "    filename = \"./metrics/confusion_\" + name + \"_\" + classifier + \".csv\"\n",
    "    confusion_mx_df.to_csv(filename)\n",
    "    \n",
    "    # Write out precision, recall, f1 by class to csv file\n",
    "    prf_by_label_list = []\n",
    "    for row in prf_by_label:\n",
    "      prf_by_label_list.append(list(row))\n",
    "    prf_by_label_df = pd.DataFrame(prf_by_label_list)\n",
    "    filename = \"./metrics/prf_by_class_\" + name + \"_\" + classifier + \".csv\"\n",
    "    prf_by_label_df.to_csv(filename)\n",
    "    \n",
    "def get_prf_by_label(name, classifier):\n",
    "    filename = \"./metrics/prf_by_class_\" + name + \"_\" + classifier + \".csv\"\n",
    "    if os.path.isfile(filename):\n",
    "        prf_by_label_df = pd.read_csv(filename)\n",
    "        return prf_by_label_df[prf_by_label_df.columns[1:]]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def get_confusion_matrix(name, classifier):\n",
    "    filename = \"./metrics/confusion_\" + name + \"_\" + classifier + \".csv\"\n",
    "    if os.path.isfile(filename):\n",
    "        confusion_df = pd.read_csv(filename)\n",
    "        return confusion_df[confusion_df.columns[1:]]\n",
    "    else:\n",
    "        return None\n",
    "      \n",
    "def calculate_metrics(name, classifier, feature_size, predict, test_labels, \n",
    "                      elapsed_time, metrics):\n",
    "    # Get precision, recall, f1 scores\n",
    "    prf_scores          = precision_recall_fscore_support(test_labels, predict, \n",
    "                                                          average='weighted')\n",
    "    acc_score           = accuracy_score(test_labels, predict)\n",
    "    prf_by_label        = precision_recall_fscore_support(test_labels, predict, \n",
    "                                                          average=None)\n",
    "    classification_rpt  = classification_report(test_labels, predict)\n",
    "\n",
    "    # Get confusion matrix\n",
    "    conf_mx             = confusion_matrix(test_labels, predict)\n",
    "\n",
    "    metrics.append({\n",
    "     'name':               name,\n",
    "     'classifier':         classifier,\n",
    "     'feature_size':       feature_size,\n",
    "     'accuracy':           acc_score,\n",
    "     'precision':          prf_scores[0],\n",
    "     'recall':             prf_scores[1],\n",
    "     'f1':                 prf_scores[2],\n",
    "     'time':               elapsed_time \n",
    "    })\n",
    "    save_metrics(name, classifier, metrics, prf_by_label, conf_mx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BcMd3vhdby7z"
   },
   "source": [
    "## Functions for running different classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gOquprmrby75"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Logistic regression\n",
    "# \n",
    "def getBestParamsLogit(train_data, train_labels):\n",
    "    #\n",
    "    # Logistic Regression\n",
    "    #\n",
    "    lr = LogisticRegression(penalty='l2', multi_class = 'ovr', solver='liblinear', max_iter=150)\n",
    "    params = {'C': [0.1, 0.25,  0.5]}\n",
    "    logit = GridSearchCV(lr, params, cv=5,\n",
    "                         scoring='accuracy', return_train_score=True)\n",
    "\n",
    "    # Fit  training data\n",
    "    logit.fit(train_data, train_labels)  \n",
    "    # Show the best C parameter to use and the expected accuracy\n",
    "    print(' Best param:', logit.best_params_)\n",
    "    print(' Accuracy:  ', np.round(logit.best_score_, 4) )\n",
    "    \n",
    "    return logit.best_params_\n",
    "\n",
    "def run_logistic_regression(train_data, train_labels, test_data, test_labels, \n",
    "                            name, hyper_params, metrics, forcerun=False):\n",
    "  \n",
    "    existing = [record for record in metrics if record['name'] == name and record['classifier'] == 'lr']\n",
    "    if (not(forcerun) and len(existing) > 0):\n",
    "      print(\"\\nLogistic Regression (skipping)\")\n",
    "      return\n",
    "    \n",
    "    print(\"\\nLogistic Regression\", name)\n",
    "\n",
    "    start = time.process_time()\n",
    "    if name in hyper_params and 'lr' in hyper_params[name]:\n",
    "        best_params_logit = hyper_params[name]['lr']\n",
    "    else:\n",
    "        print(\"Running grid search on Logistic Regression...\")\n",
    "        best_params_logit = getBestParamsLogit(train_data, train_labels)\n",
    "\n",
    "    # Run logistic regression with L2 regularization on reduced\n",
    "    # feature set\n",
    "    lr = LogisticRegression(penalty='l2', tol=.01, max_iter=150, \n",
    "                          C=best_params_logit['C'], \n",
    "                          solver=\"liblinear\", multi_class=\"ovr\")\n",
    "    lr.fit(train_data, train_labels) \n",
    "    predict = lr.predict(test_data)\n",
    "    elapsed_time = time.process_time() - start\n",
    "\n",
    "\n",
    "    calculate_metrics(name, 'lr', train_data.shape[1], predict, test_labels, \n",
    "                      elapsed_time, metrics)\n",
    "\n",
    "    print(\" done.\")\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KHfO2X_Vby77"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Linear SVM\n",
    "#\n",
    "\n",
    "def getBestParamsSVM(train_data, train_labels):\n",
    "    #\n",
    "    # SVM\n",
    "    #\n",
    "    classifier = LinearSVC(penalty='l2')\n",
    "\n",
    "    params = {'C': [0.01, 0.1, 0.5]}\n",
    "    svm = GridSearchCV(classifier, params, cv=4, \n",
    "                       scoring='accuracy', return_train_score=True)\n",
    "\n",
    "    # Fit  training data\n",
    "    svm.fit(train_data, train_labels)  \n",
    "    # Show the best C parameter to use and the expected accuracy\n",
    "    print(' Best param:', svm.best_params_)\n",
    "    print(' Accuracy:  ', np.round(svm.best_score_, 4) )\n",
    "    \n",
    "    return svm.best_params_\n",
    "  \n",
    "def run_linear_svm(train_data, train_labels, test_data, test_labels, \n",
    "                   name, hyper_params, metrics, forcerun=False):\n",
    "    \n",
    "    existing = [record for record in metrics if record['name'] == name and record['classifier'] == 'svm']\n",
    "    if (not(forcerun) and len(existing) > 0):\n",
    "      print(\"\\nLinear SVM (skipping)\")\n",
    "      return\n",
    "    \n",
    "    print(\"\\nLinear SVM\", name)\n",
    "    start = time.process_time()\n",
    "    if name in hyper_params and 'svm' in hyper_params[name]:\n",
    "        best_params_svm = hyper_params[name]['svm']\n",
    "    else:\n",
    "        print(\"Running grid search on Linear SVM...\")\n",
    "        best_params_svm = getBestParamsSVM(train_data, train_labels)\n",
    "\n",
    "    svm = LinearSVC(penalty='l2', C=best_params_svm['C'])\n",
    "\n",
    "    svm.fit(train_data, train_labels,) \n",
    "    predict = svm.predict(test_data)\n",
    "    elapsed_time = time.process_time() - start\n",
    "\n",
    "    calculate_metrics(name, 'svm', train_data.shape[1], predict, test_labels, \n",
    "                      elapsed_time, metrics)\n",
    "    \n",
    "    print(\" done.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "muZmidvcby8A"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Decision tree\n",
    "#\n",
    "def run_decision_tree(train_data, train_labels, test_data, test_labels, name, hyper_params, metrics):\n",
    "\n",
    "    existing = [record for record in metrics if record['name'] == name and record['classifier'] == 'dt']\n",
    "    if (len(existing) > 0):\n",
    "      print(\"\\nDecision Tree (skipping)\")\n",
    "      return\n",
    "    \n",
    "    print(\"\\nDecision Tree\", name)\n",
    "    start = time.process_time()\n",
    "    dt = DecisionTreeClassifier()\n",
    "    \n",
    "    dt.fit(train_data, train_labels,) \n",
    "    predict = dt.predict(test_data)\n",
    "    elapsed_time = time.process_time() - start\n",
    "\n",
    "\n",
    "    calculate_metrics(name, 'dt', train_data.shape[1], predict, test_labels, \n",
    "                      elapsed_time, metrics)\n",
    "    \n",
    "    print(\" done.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aK-uq9g0by8C"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Random forest\n",
    "#\n",
    "def run_random_forest(train_data, train_labels, test_data, test_labels, name, hyper_params, metrics):\n",
    "  \n",
    "    existing = [record for record in metrics if record['name'] == name and record['classifier'] == 'rf']\n",
    "    if (len(existing) > 0):\n",
    "      print(\"\\nRandom Forest (skipping)\")\n",
    "      return\n",
    "    \n",
    "    print(\"\\nRandom Forest\", name)\n",
    "    start = time.process_time()\n",
    "    rf = RandomForestClassifier(n_estimators=500)\n",
    "    \n",
    "    rf.fit(train_data, train_labels,) \n",
    "    predict = rf.predict(test_data)\n",
    "    elapsed_time = time.process_time() - start\n",
    "\n",
    "    calculate_metrics(name, 'rf', train_data.shape[1], predict, test_labels, \n",
    "                      elapsed_time, metrics)\n",
    "    \n",
    "    print(\" done.\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W7u0wd8oby8F"
   },
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# Neural Net\n",
    "#\n",
    "def run_neural_net(train_data, train_labels, test_data, test_labels, name, hyper_params, metrics):\n",
    "    existing = [record for record in metrics if record['name'] == name and record['classifier'] == 'nn']\n",
    "    if (len(existing) > 0):\n",
    "      print(\"\\nNeural Net (skipping)\")\n",
    "      return\n",
    "    \n",
    "    print(\"\\nNeural Net\", name)\n",
    "    tr_lab = to_categorical(train_labels)\n",
    "    test_lab = to_categorical(test_labels)\n",
    "    \n",
    "    number_of_classes = len(tr_lab[0])\n",
    "    \n",
    "    start = time.process_time()\n",
    "    model = K.Sequential()\n",
    "    model.add(Dense(2000, input_dim=train_data.shape[1], activation='relu', \n",
    "                    kernel_regularizer=regularizers.l1_l2(l2=0.01,l1=0.01)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1000, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(400, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(number_of_classes, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = [\"accuracy\"])\n",
    "    \n",
    "    \n",
    "    #model.fit(train_data, tr_lab, epochs=200, batch_size=100)\n",
    "    if name == 'after_pca':\n",
    "      n_epochs = 10\n",
    "    else:\n",
    "      n_epochs = 140\n",
    "    \n",
    "\n",
    "    hist = model.fit(train_data, tr_lab, epochs=n_epochs, batch_size=100,\n",
    "                     validation_data=(test_data, test_lab))\n",
    "    \n",
    "    \n",
    "    evaluate = model.evaluate(x = test_data, y = test_lab)\n",
    "    predict = model.predict(test_data)    \n",
    "    \n",
    "    elapsed_time = time.process_time() - start\n",
    "\n",
    "    calculate_metrics(name, 'nn', train_data.shape[1],\n",
    "                      np.argmax(predict,1), test_labels, elapsed_time, metrics)\n",
    "\n",
    "    print(\" done.\")\n",
    "    \n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dwdypavvby8K"
   },
   "source": [
    "## Run the different classifiers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MPlio9KCby8R",
    "outputId": "1ffef275-ec68-42ea-bdc5-0679fb395165"
   },
   "outputs": [],
   "source": [
    "hyper_params = {\n",
    "    'l1reg_c0.025':         {'lr': {'C': 0.25}, 'svm': {'C': 0.01}}, \n",
    "    'l1reg_c0.05':          {'lr': {'C': 0.25}, 'svm': {'C': 0.01}}, \n",
    "    'l1reg_c0.1':           {'lr': {'C': 0.25}, 'svm': {'C': 0.01}}, \n",
    "    'l1reg_c0.25':           {'lr': {'C': 0.25}, 'svm': {'C': 0.01}}, \n",
    "    'l1reg_c0.5':           {'lr': {'C': 0.25}, 'svm': {'C': 0.01}},\n",
    "    'l1reg_c1':             {'lr': {'C': 0.25}, 'svm': {'C': 0.01}},\n",
    "    'l1reg_c10':            {'lr': {'C': 0.1},  'svm': {'C': 0.01}},\n",
    "    'l1reg_c100':           {'lr': {'C': 0.25}, 'svm': {'C': 0.01}},\n",
    "    \n",
    "    'top_100_genes':        {'lr': {'C': 0.1},  'svm': {'C': 0.01}},\n",
    "\n",
    "    'bestfit_med':          {'lr': {'C': 0.1 }, 'svm': {'C': 0.01}},\n",
    "    'bestfit_large':        {'lr': {'C': 0.1 }, 'svm': {'C': 0.01}},\n",
    "    \n",
    "    'all':                  {'lr': {'C': 0.25}, 'svm': {'C': 0.01}},\n",
    "    'after_pca':            {'lr': {'C': 0.5 }, 'svm': {'C': 0.01}},\n",
    "    \n",
    "    'rfe_svm_100':          {'lr': {'C': 0.5},  'svm': {'C': 0.1}}, \n",
    "    'rfe_lr_100':           {'lr': {'C': 0.5},  'svm': {'C': 0.1}}, \n",
    "\n",
    "    'rfe_svm_800':          {'lr': {'C': 0.5},  'svm': {'C': 0.1}}, \n",
    "    'rfe_lr_800':           {'lr': {'C': 0.1},  'svm': {'C': 0.01}}, \n",
    "    \n",
    "    'rfe_svm_4000':         {'lr': {'C': 0.5},  'svm': {'C': 0.01}}, \n",
    "    'rfe_lr_4000':          {'lr': {'C': 0.1},  'svm': {'C': 0.01}}, \n",
    "    \n",
    "    'rfe_svm_8000':         {'lr': {'C': 0.25}, 'svm': {'C': 0.01}}, \n",
    "    'rfe_lr_8000':          {'lr': {'C': 0.1},  'svm': {'C': 0.01}} \n",
    "}\n",
    "\n",
    "metrics = get_saved_metrics()\n",
    "\n",
    "\n",
    "for name in all_train_data.keys():\n",
    "    print(\"************************\")\n",
    "    print(name)\n",
    "    print(\"************************\")\n",
    "\n",
    "    train      = all_train_data[name]\n",
    "    test       = all_test_data[name]\n",
    "    \n",
    "    \n",
    "\n",
    "    run_logistic_regression(train['data'], train['labels'], test['data'], test['labels'], \n",
    "                            name, hyper_params, metrics, forcerun=False)\n",
    "    \n",
    "    run_linear_svm(train['data'], train['labels'], test['data'], test['labels'], \n",
    "                   name, hyper_params, metrics, forcerun=False)\n",
    "\n",
    "    run_decision_tree(train['data'], train['labels'], test['data'], test['labels'], \n",
    "                   name, hyper_params, metrics)\n",
    "    \n",
    "    run_random_forest(train['data'], train['labels'], test['data'], test['labels'], \n",
    "                   name, hyper_params, metrics)\n",
    "    \n",
    "    run_neural_net(train['data'], train['labels'], test['data'], test['labels'], \n",
    "                   name, hyper_params, metrics)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "disera_nair_singh_fraenkel_final_project.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
