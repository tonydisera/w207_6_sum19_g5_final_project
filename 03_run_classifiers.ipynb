{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from textwrap import wrap\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TCGA dictionary information\n",
    "tcga_dict = open(\"./data/tcga_dictionaries.txt\",\"r\")\n",
    "dict_name_index = 0 #Set dictionary index counter to 0\n",
    "for line in tcga_dict:\n",
    "    if line.startswith(\"#\"): #If line starts with #, the next line will be a known dictionary\n",
    "        dict_name_index += 1\n",
    "    elif dict_name_index == 5:\n",
    "        code_to_disease = eval(line)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataAndLabels(name, features):\n",
    "    labels_string = features.cancer_type\n",
    "    le            = preprocessing.LabelEncoder()\n",
    "    labels        = le.fit_transform(labels_string)\n",
    "\n",
    "    # Get rid of the cancer type and patient_id columns \n",
    "    data = features[features.columns[3:]]\n",
    "    return {'name': name, 'feature_size': data.shape[1],\n",
    "            'data': data, 'labels': labels , 'label_encoder': le }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading training data ...')\n",
    "\n",
    "train_files = glob.glob(\"./data/features_*.train.csv\")\n",
    "all_train_data = {}\n",
    "for filename in train_files:\n",
    "    \n",
    "    name = filename[16:-10]\n",
    "    print(\" \", name)\n",
    "    train_features = pd.read_csv(filename)\n",
    "    all_train_data[name] = getDataAndLabels(name, train_features)\n",
    "\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading test data ...')\n",
    "\n",
    "test_files = glob.glob(\"./data/features_*.test.csv\")\n",
    "all_test_data = {}\n",
    "for filename in test_files:\n",
    "    \n",
    "    name = filename[16:-9]\n",
    "    print(\" \", name)\n",
    "    test_features = pd.read_csv(filename)\n",
    "    all_test_data[name] = getDataAndLabels(name, test_features)\n",
    "\n",
    "print(\"done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestParamsLogit(train_data, train_labels):\n",
    "    #\n",
    "    # Logistic Regression\n",
    "    #\n",
    "    lr = LogisticRegression(penalty='l2', multi_class = 'ovr', solver='liblinear', max_iter=150)\n",
    "    params = {'C': [0.1, 0.25,  0.5,]}\n",
    "    logit = GridSearchCV(lr, params, cv=5,\n",
    "                         scoring='accuracy', return_train_score=True)\n",
    "\n",
    "    # Fit  training data\n",
    "    logit.fit(train_data, train_labels)  \n",
    "    # Show the best C parameter to use and the expected accuracy\n",
    "    print(' Best param:', logit.best_params_)\n",
    "    print(' Accuracy:  ', np.round(logit.best_score_, 4) )\n",
    "    \n",
    "    return logit.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBestParamsSVM(train_data, train_labels):\n",
    "    #\n",
    "    # SVM\n",
    "    #\n",
    "    classifier = LinearSVC(penalty='l2')\n",
    "\n",
    "    params = {'C': [0.01, 0.1, 0.5]}\n",
    "    svm = GridSearchCV(classifier, params, cv=4, \n",
    "                       scoring='accuracy', return_train_score=True)\n",
    "\n",
    "    # Fit  training data\n",
    "    svm.fit(train_data, train_labels)  \n",
    "    # Show the best C parameter to use and the expected accuracy\n",
    "    print(' Best param:', svm.best_params_)\n",
    "    print(' Accuracy:  ', np.round(svm.best_score_, 4) )\n",
    "    \n",
    "    return svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runClassifiers(train_data, train_labels, test_data, test_labels, name, hyper_params, scores):\n",
    "\n",
    "    if name in hyper_params and 'lr' in hyper_params[name]:\n",
    "        best_params_logit = hyper_params[name]['lr']\n",
    "    else:\n",
    "        print(\"Running grid search on Logistic Regression...\")\n",
    "        best_params_logit = getBestParamsLogit(train_data, train_labels)\n",
    "\n",
    "    if name in hyper_params and 'svm' in hyper_params[name]:\n",
    "        best_params_svm = hyper_params[name]['svm']\n",
    "    else:\n",
    "        print(\"Running grid search on Linear SVM...\")\n",
    "        best_params_svm = getBestParamsSVM(train_data, train_labels)\n",
    "\n",
    "\n",
    "    # Run logistic regression with L2 regularization on reduced\n",
    "    # feature set\n",
    "    lr = LogisticRegression(penalty='l2', tol=.01, max_iter=150, \n",
    "                            C=best_params_logit['C'], \n",
    "                            solver=\"liblinear\", multi_class=\"ovr\")\n",
    "    lr.fit(train_data, train_labels) \n",
    "    predict = lr.predict(test_data)\n",
    "\n",
    "    # Get precision, recall, f1 scores\n",
    "    logit_prf_scores      = precision_recall_fscore_support(test_labels, predict, average='weighted')\n",
    "    logit_scores_by_label = precision_recall_fscore_support(test_labels, predict, average=None)\n",
    "\n",
    "    # Get confusion matrix\n",
    "    logit_confusion       = confusion_matrix(test_labels, predict)\n",
    "\n",
    "        \n",
    "    #\n",
    "    # Run Linear SVM\n",
    "    #\n",
    "    svm = LinearSVC(penalty='l2', C=best_params_svm['C'])\n",
    "\n",
    "    svm.fit(train_data, train_labels,) \n",
    "    predict = svm.predict(test_data)\n",
    "\n",
    "    # Get precision, recall, f1 scores\n",
    "    svm_prf_scores      = precision_recall_fscore_support(test_labels, predict, average='weighted')\n",
    "    svm_scores_by_label = precision_recall_fscore_support(test_labels, predict, average=None)\n",
    "\n",
    "    # Get confusion matrix\n",
    "    svm_confusion       = confusion_matrix(test_labels, predict)\n",
    "\n",
    "    print(\"\\nLogistic Regression\", name)\n",
    "    print(\"  precision:\", np.round(logit_prf_scores[0], 4))  \n",
    "    print(\"  recall:   \", np.round(logit_prf_scores[1], 4))  \n",
    "    print(\"  f1:       \", np.round(logit_prf_scores[2], 4))  \n",
    "\n",
    "    print(\"\\nLinear SVM\", name)\n",
    "    print(\"  precision:\", np.round(svm_prf_scores[0], 4))  \n",
    "    print(\"  recall:   \", np.round(svm_prf_scores[1], 4))  \n",
    "    print(\"  f1:       \", np.round(svm_prf_scores[2], 4))  \n",
    "    \n",
    "    scores[name] = {\n",
    "        'lr': [\n",
    "            logit_prf_scores[0],\n",
    "            logit_prf_scores[1],\n",
    "            logit_prf_scores[2],\n",
    "            logit_scores_by_label,\n",
    "            logit_confusion\n",
    "        ],\n",
    "        'svm': [\n",
    "            svm_prf_scores[0],\n",
    "            svm_prf_scores[1],\n",
    "            svm_prf_scores[2],\n",
    "            svm_scores_by_label,\n",
    "            svm_confusion\n",
    "        ]\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder            = preprocessing.LabelEncoder()\n",
    "\n",
    "hyper_params = {\n",
    "    'l1reg_c0.5':           {'lr': {'C': 0.25}, 'svm': {'C': 0.01}},\n",
    "    'l1reg_c1':             {'lr': {'C': 0.25}, 'svm': {'C': 0.01}},\n",
    "    'l1reg_c10':            {'lr': {'C': 0.1},  'svm': {'C': 0.01}},\n",
    "    'l1reg_c100':           {'lr': {'C': 0.25}, 'svm': {'C': 0.01}},\n",
    "    'topgenes_small':       {'lr': {'C': 0.25}, 'svm': {'C': 0.01}},\n",
    "    'bestfit_med':          {'lr': {'C': 0.1 }, 'svm': {'C': 0.01}},\n",
    "    'bestfit_large':        {'lr': {'C': 0.1 }, 'svm': {'C': 0.01}},\n",
    "    'all':                  {'lr': {'C': 0.25}, 'svm': {'C': 0.01}},\n",
    "    'bestfit_with_topgenes':{'lr': {'C': 0.1 }, 'svm': {'C': 0.01}}\n",
    "}\n",
    "\n",
    "\n",
    "scores = {}\n",
    "\n",
    "\n",
    "for name in all_train_data.keys():\n",
    "    print(\"************************\")\n",
    "    print(name)\n",
    "    print(\"************************\")\n",
    "\n",
    "    train      = all_train_data[name]\n",
    "    test       = all_test_data[name]\n",
    "\n",
    "    runClassifiers(train['data'], train['labels'], test['data'], test['labels'], name, hyper_params, scores)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores = pd.DataFrame(scores)\n",
    "rows = []\n",
    "for name in all_train_data.keys():    \n",
    "    rows.append([name,\n",
    "                 all_train_data[name]['feature_size'],\n",
    "                'lr',\n",
    "                df_scores.loc['lr'][name][0],\n",
    "                df_scores.loc['lr'][name][1],\n",
    "                df_scores.loc['lr'][name][2]])\n",
    "for name in all_train_data.keys():    \n",
    "    rows.append([name,\n",
    "                 all_train_data[name]['feature_size'],\n",
    "                'svm',\n",
    "                df_scores.loc['svm'][name][0],\n",
    "                df_scores.loc['svm'][name][1],\n",
    "                df_scores.loc['svm'][name][2]])\n",
    "\n",
    "df_report = pd.DataFrame(rows, columns=['name', 'feature_size', 'classifier', 'precision', 'recall', 'f1'])\n",
    "display(df_report)\n",
    "\n",
    "# best precision\n",
    "sorted_df = df_report.sort_values(by='precision', ascending=0)\n",
    "display(sorted_df.head(1))\n",
    "\n",
    "# best recall\n",
    "sorted_df = df_report.sort_values(by='recall', ascending=0)\n",
    "display(sorted_df.head(1))\n",
    "\n",
    "# best f1\n",
    "sorted_df = df_report.sort_values(by='f1', ascending=0)\n",
    "display(sorted_df.head(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
